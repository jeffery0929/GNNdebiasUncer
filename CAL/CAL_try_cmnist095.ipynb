{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1SzKE9PQ2tu2Fi6ADjwzPog_SE68LM94J","authorship_tag":"ABX9TyNfmvPbXbxm7D7V5B5aIVTp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xVEXjGk1WGRp","executionInfo":{"status":"ok","timestamp":1684722944358,"user_tz":-720,"elapsed":42354,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}},"outputId":"88b3de81-e039-49d2-a627-95c9f4172ab4"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.0.1+cu118\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\n","Collecting pyg_lib\n","  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/pyg_lib-0.2.0%2Bpt20cpu-cp310-cp310-linux_x86_64.whl (627 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m627.0/627.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_scatter\n","  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_scatter-2.1.1%2Bpt20cpu-cp310-cp310-linux_x86_64.whl (504 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.1/504.1 kB\u001b[0m \u001b[31m921.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_sparse\n","  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_sparse-0.6.17%2Bpt20cpu-cp310-cp310-linux_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_cluster\n","  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_cluster-1.6.1%2Bpt20cpu-cp310-cp310-linux_x86_64.whl (732 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m732.3/732.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_spline_conv\n","  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_spline_conv-1.2.2%2Bpt20cpu-cp310-cp310-linux_x86_64.whl (205 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.7/205.7 kB\u001b[0m \u001b[31m607.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.10.1)\n","Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch_sparse) (1.22.4)\n","Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n","Successfully installed pyg_lib-0.2.0+pt20cpu torch_cluster-1.6.1+pt20cpu torch_scatter-2.1.1+pt20cpu torch_sparse-0.6.17+pt20cpu torch_spline_conv-1.2.2+pt20cpu\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement warnings (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for warnings\u001b[0m\u001b[31m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting dgl\n","  Downloading dgl-1.1.0-cp310-cp310-manylinux1_x86_64.whl (5.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.22.4)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.10.1)\n","Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.65.0)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4)\n","Installing collected packages: dgl\n","Successfully installed dgl-1.1.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting texttable\n","  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n","Installing collected packages: texttable\n","Successfully installed texttable-1.6.7\n","cpu\n"]}],"source":["import os\n","import torch\n","os.environ['TORCH'] = torch.__version__\n","print(torch.__version__)\n","\n","#!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n","#!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n","!pip install warnings\n","!pip install dgl\n","!pip install texttable\n","\n","if torch.cuda.is_available():\n","  device = torch.device(\"cuda\")\n","else:\n","  device = torch.device(\"cpu\")\n","print(device)"]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dq-Zd6aQWRvb","executionInfo":{"status":"ok","timestamp":1684722944359,"user_tz":-720,"elapsed":11,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}},"outputId":"b86e0a7f-8191-4f65-805e-54572b2851fc"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":[],"metadata":{"id":"cGqoLa84WV7k","executionInfo":{"status":"ok","timestamp":1684722944360,"user_tz":-720,"elapsed":6,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"LtHqVCYeNmmv","executionInfo":{"status":"ok","timestamp":1684722976769,"user_tz":-720,"elapsed":32414,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"462834eb-9599-4b5f-cbe1-bf4ad762455c"},"outputs":[{"output_type":"stream","name":"stderr","text":["DGL backend not selected or invalid.  Assuming PyTorch for now.\n"]},{"output_type":"stream","name":"stdout","text":["Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"]}],"source":["import random\n","from torchvision import transforms, datasets\n","\n","import os\n","import pickle\n","from scipy.spatial.distance import cdist\n","from scipy import ndimage\n","import numpy as np\n","\n","import dgl\n","import torch\n","import time\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","import matplotlib\n","def sigma(dists, kth=8):\n","    # Get k-nearest neighbors for each node\n","    knns = np.partition(dists, kth, axis=-1)[:, kth::-1]\n","\n","    # Compute sigma and reshape\n","    sigma = knns.sum(axis=1).reshape((knns.shape[0], 1))/kth\n","    return sigma + 1e-8 # adding epsilon to avoid zero value of sigma\n","\n","def compute_adjacency_matrix_images(coord, feat, use_feat=False, kth=8):\n","    coord = coord.reshape(-1, 2)\n","    # Compute coordinate distance\n","    c_dist = cdist(coord, coord)\n","    \n","    if use_feat:\n","        # Compute feature distance\n","        f_dist = cdist(feat, feat)\n","        # Compute adjacency\n","        A = np.exp(- (c_dist/sigma(c_dist))**2 - (f_dist/sigma(f_dist))**2 )\n","    else:\n","        A = np.exp(- (c_dist/sigma(c_dist))**2)\n","        \n","    # Convert to symmetric matrix\n","    A = 0.5 * A * A.T\n","    A[np.diag_indices_from(A)] = 0\n","    return A\n","\n","def compute_edges_list(A, kth=8+1):\n","    # Get k-similar neighbor indices for each node\n","    if 1==1:   \n","        num_nodes = A.shape[0]\n","        new_kth = num_nodes - kth\n","        knns = np.argpartition(A, new_kth-1, axis=-1)[:, new_kth:-1]\n","        knns_d = np.partition(A, new_kth-1, axis=-1)[:, new_kth:-1]\n","    else:\n","        knns = np.argpartition(A, kth, axis=-1)[:, kth::-1]\n","        knns_d = np.partition(A, kth, axis=-1)[:, kth::-1]\n","    return knns, knns_d\n","class newCIFARSuperPix(torch.utils.data.Dataset):\n","    def __init__(self,\n","                 data_dir,\n","                 use_mean_px=True,\n","                 use_coord=True,\n","                 use_feat_for_graph_construct=False,):\n","\n","        #self.split = split\n","        #self.is_test = split.lower() in ['test', 'val']\n","        with open(data_dir, 'rb') as f:\n","            self.labels, self.sp_data = pickle.load(f)\n","\n","        self.use_mean_px = use_mean_px\n","        self.use_feat_for_graph = use_feat_for_graph_construct\n","        self.use_coord = use_coord\n","        self.n_samples = len(self.labels)\n","        self.img_size = 32\n","\n","    def precompute_graph_images(self):\n","        #print('precompute all data for the %s set...' % self.split.upper())\n","        self.Adj_matrices, self.node_features, self.edges_lists = [], [], []\n","        for index, sample in enumerate(self.sp_data):\n","            mean_px, coord = sample[:2]\n","            coord = coord / self.img_size\n","            A = compute_adjacency_matrix_images(coord, mean_px, use_feat=self.use_feat_for_graph)\n","            edges_list, _ = compute_edges_list(A)\n","            N_nodes = A.shape[0]\n","            \n","            x = None\n","            if self.use_mean_px:\n","                x = mean_px.reshape(N_nodes, -1)\n","            if self.use_coord:\n","                coord = coord.reshape(N_nodes, 2)\n","                if self.use_mean_px:\n","                    x = np.concatenate((x, coord), axis=1)\n","                else:\n","                    x = coord\n","            if x is None:\n","                x = np.ones(N_nodes, 1)  # dummy features\n","            \n","            self.node_features.append(x)\n","            self.Adj_matrices.append(A)\n","            self.edges_lists.append(edges_list)\n","\n","    def __len__(self):\n","        return self.n_samples\n","\n","    def __getitem__(self, index):\n","        g = dgl.DGLGraph()\n","        g.add_nodes(self.node_features[index].shape[0])\n","        g.ndata['feat'] = torch.Tensor(self.node_features[index])\n","        for src, dsts in enumerate(self.edges_lists[index]):\n","            g.add_edges(src, dsts[dsts!=src])\n","\n","        return g, self.labels[index]\n","\n","use_feat_for_graph_construct = False\n","tt = time.time()\n","data_with_feat_knn = newCIFARSuperPix(\"/content/drive/MyDrive/CMINST_data/CMNIST095_60000_75sp_train.pkl\",use_feat_for_graph_construct=use_feat_for_graph_construct)\n","\n","data_with_feat_knn.precompute_graph_images()\n","training_data = np.load('/content/drive/MyDrive/CMINST_data/colorMNIST095_60000_data.npy')\n","training_label=np.load('/content/drive/MyDrive/CMINST_data/colorMNIST095_60000_label.npy')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"G0eqa0s54oKO","executionInfo":{"status":"ok","timestamp":1684722978868,"user_tz":-720,"elapsed":2122,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}}},"outputs":[],"source":["import numpy as np\n","import os.path as osp\n","import pickle\n","import torch\n","import torch.utils\n","import torch.utils.data\n","import torch.nn.functional as F\n","from scipy.spatial.distance import cdist\n","from torch_geometric.utils import dense_to_sparse\n","from torch_geometric.data import InMemoryDataset,Data\n","#/content/drive/MyDrive/Colab_Notebooks/mnist08_83_75sp_train.pkl\n","def compute_adjacency_matrix_images(coord, sigma=0.1):\n","    coord = coord.reshape(-1, 2)\n","    dist = cdist(coord, coord)\n","    A = np.exp(- dist / (sigma * np.pi) ** 2)\n","    A[np.diag_indices_from(A)] = 0\n","    return A\n","\n","\n","def list_to_torch(data):\n","    for i in range(len(data)):\n","        if data[i] is None:\n","            continue\n","        elif isinstance(data[i], np.ndarray):\n","            if data[i].dtype == np.bool:\n","                data[i] = data[i].astype(np.float32)\n","            data[i] = torch.from_numpy(data[i]).float()\n","        elif isinstance(data[i], list):\n","            data[i] = list_to_torch(data[i])\n","    return data\n","def process(data_file):\n","  use_mean_px=True\n","  use_coord=True\n","  node_gt_att_threshold=0\n","  transform=None\n","  pre_transform=None\n","  pre_filter=None\n","\n","  #data_file ='/content/drive/MyDrive/Colab_Notebooks/colorMNIST05_2000_75sp_train.pkl' \n","\n","  with open(osp.join(data_file), 'rb') as f:\n","      labels,sp_data = pickle.load(f)\n","      \n","  #use_mean_px = self.use_mean_px\n","  #self.use_coord = self.use_coord\n","  n_samples = len(labels)\n","  img_size = 32\n","  #node_gt_att_threshold = self.node_gt_att_threshold\n","\n","  edge_indices,xs,edge_attrs,node_gt_atts,edge_gt_atts = [], [], [], [], []\n","  data_list = []\n","  for index, sample in enumerate(sp_data):\n","      mean_px, coord = sample[:2]\n","      coord = coord / img_size\n","      A = compute_adjacency_matrix_images(coord)\n","      N_nodes = A.shape[0]\n","      \n","      A = torch.FloatTensor((A > 0.1) * A)\n","      edge_index, edge_attr = dense_to_sparse(A)\n","\n","      x = None\n","      if use_mean_px:\n","          x = mean_px.reshape(N_nodes, -1)\n","      if use_coord:\n","          coord = coord.reshape(N_nodes, 2)\n","          if use_mean_px:\n","              x = np.concatenate((x, coord), axis=1)\n","          else:\n","              x = coord\n","      if x is None:\n","          x = np.ones(N_nodes, 1)  # dummy features\n","          \n","      # replicate features to make it possible to test on colored images\n","      x = np.pad(x, ((0, 0), (2, 0)), 'edge')  \n","      if node_gt_att_threshold == 0:\n","          node_gt_att = (mean_px > 0).astype(np.float32)\n","      else:\n","          node_gt_att = mean_px.copy()\n","          node_gt_att[node_gt_att < node_gt_att_threshold] = 0\n","\n","      node_gt_att = torch.LongTensor(node_gt_att).view(-1)\n","      row, col = edge_index\n","      edge_gt_att = torch.LongTensor(node_gt_att[row] * node_gt_att[col]).view(-1)\n","\n","      data_list.append(\n","          Data(\n","              x=torch.tensor(x), \n","              y=torch.LongTensor([labels[index]]), \n","              edge_index=edge_index,\n","              edge_attr=edge_attr, \n","              node_gt_att=node_gt_att,\n","              edge_gt_att=edge_gt_att\n","\n","          )\n","      )\n","\n","  #torch.save(InMemoryDataset.collate(data_list), '/content/drive/MyDrive/Colab_Notebooks/colorMINST05_2000.pt')\n","  return data_list"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"JrQnfDYkxdR7","executionInfo":{"status":"ok","timestamp":1684722998889,"user_tz":-720,"elapsed":20024,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}}},"outputs":[],"source":["train_dir='/content/drive/MyDrive/CMINST_data/CMNIST095_10000_75sp_train.pkl'\n","val_dir='/content/drive/MyDrive/CMINST_data/CMNIST5000_75sp_val.pkl'\n","test_dir='/content/drive/MyDrive/CMINST_data/CMNIST10000_75sp_test.pkl'\n","training_final=process(data_file=train_dir)\n","valing_final=process(data_file=val_dir)\n","testing_final=process(data_file=test_dir)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Q2RkYPa8k9nE","executionInfo":{"status":"ok","timestamp":1684722998891,"user_tz":-720,"elapsed":24,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}}},"outputs":[],"source":["import torch\n","from torch.nn import Parameter\n","from torch_scatter import scatter_add\n","from torch_geometric.nn.conv import MessagePassing\n","from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n","from torch_geometric.nn.inits import glorot, zeros\n","import pdb\n","def mask_graph(graph_x,select_node):\n","  mask_value=np.array([0.0001]*32)\n","  result=np.array([t.detach().numpy() for t in graph_x])\n","  for i in range(graph_x.shape[0]):\n","    if(i in select_node):\n","      continue\n","    result[i]=mask_value\n","  return torch.tensor(result)\n","\n","\n","\n","\n","\n","def split_graph(graph_x,node_of_graph,type_of_graph=True):\n","  if(type_of_graph==True):\n","    select_node_number=int(node_of_graph/3)\n","    select_node=torch.topk(graph_x.mean(axis=1),select_node_number)[1]\n","    #print(select_node)\n","    return mask_graph(graph_x,select_node),select_node\n","  else:\n","    select_node_number=int(node_of_graph/3*2)\n","    select_node=torch.topk(graph_x.mean(axis=1),select_node_number)[1]\n","    return mask_graph(graph_x,select_node),select_node\n","\n","def uncertainty_mask_gnerate(node_in_graph,number_of_mask):\n","  all_mask=[]\n","  for i in range(number_of_mask):\n","    random_mask=random.sample(node_in_graph.tolist(),int(len(node_in_graph)/3))\n","    all_mask.append(random_mask)\n","  return all_mask\n","def mean(l):\n","  return sum(l)/len(l)\n","class GCNConv(MessagePassing):\n","    \n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 improved=False,\n","                 cached=False,\n","                 bias=True,\n","                 edge_norm=True,\n","                 gfn=False):\n","        super(GCNConv, self).__init__('add')\n","\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.improved = improved\n","        self.cached = cached\n","        self.cached_result = None\n","        self.edge_norm = edge_norm\n","        self.gfn = gfn\n","        self.message_mask = None\n","        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n","\n","        if bias:\n","            self.bias = Parameter(torch.Tensor(out_channels))\n","        else:\n","            self.register_parameter('bias', None)\n","\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        glorot(self.weight)\n","        zeros(self.bias)\n","        self.cached_result = None\n","\n","    @staticmethod\n","    def norm(edge_index, num_nodes, edge_weight, improved=False, dtype=None):\n","        if edge_weight is None:\n","            edge_weight = torch.ones((edge_index.size(1), ),\n","                                     dtype=dtype,\n","                                     device=edge_index.device)\n","        \n","        edge_weight = edge_weight.view(-1)\n","        \n","        \n","        assert edge_weight.size(0) == edge_index.size(1)\n","        \n","        edge_index, edge_weight = remove_self_loops(edge_index, edge_weight)\n","        edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n","        # Add edge_weight for loop edges.\n","        loop_weight = torch.full((num_nodes, ),\n","                                 1 if not improved else 2,\n","                                 dtype=edge_weight.dtype,\n","                                 device=edge_weight.device)\n","        edge_weight = torch.cat([edge_weight, loop_weight], dim=0)\n","\n","        row, col = edge_index\n","        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n","        deg_inv_sqrt = deg.pow(-0.5)\n","        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n","        \n","        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n","\n","    def forward(self, x, edge_index, edge_weight=None):\n","        \"\"\"\"\"\"\n","        \n","        x = torch.matmul(x, self.weight)\n","        if self.gfn:\n","            return x\n","    \n","        if not self.cached or self.cached_result is None:\n","            if self.edge_norm:\n","                edge_index, norm = GCNConv.norm(\n","                    edge_index, \n","                    x.size(0), \n","                    edge_weight, \n","                    self.improved, \n","                    x.dtype)\n","            else:\n","                norm = None\n","            self.cached_result = edge_index, norm\n","\n","        edge_index, norm = self.cached_result\n","        return self.propagate(edge_index, x=x, norm=norm)\n","\n","    def message(self, x_j, norm):\n","\n","        if self.edge_norm:\n","            return norm.view(-1, 1) * x_j\n","        else:\n","            return x_j\n","        \n","    def update(self, aggr_out):\n","        if self.bias is not None:\n","            aggr_out = aggr_out + self.bias\n","        return aggr_out\n","\n","    def __repr__(self):\n","        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n","                                   self.out_channels)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"jCXZmqIT4gvJ","executionInfo":{"status":"ok","timestamp":1684722998891,"user_tz":-720,"elapsed":22,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}}},"outputs":[],"source":["from functools import partial\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import Linear, BatchNorm1d, Sequential, ReLU\n","from torch_geometric.nn import global_mean_pool, global_add_pool, GINConv, GATConv\n","\n","import random\n","import pdb\n","device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n","\n","layers=3\n","with_random=True\n","fc_num=222\n","hidden=32\n","eval_random=False\n","class causalpreCO(torch.nn.Module):\n","  def __init__(self, num_features,num_classes,gfn=False,collapse=False,residual=False,\n","                      res_branch=\"BNConvReLU\", \n","                      global_pool=\"sum\", \n","                      dropout=0, \n","                      edge_norm=True):\n","    super(causalpreCO, self).__init__()\n","    num_conv_layers = layers\n","    self.global_pool = global_add_pool\n","\n","    self.with_random = with_random\n","    GConv = partial(GCNConv, edge_norm=edge_norm, gfn=gfn)\n","    self.bnc = BatchNorm1d(hidden)\n","    self.bno= BatchNorm1d(hidden)\n","    self.context_convs = GConv(hidden, hidden)\n","    self.objects_convs = GConv(hidden, hidden)\n","\n","\n","    hidden_in = num_features\n","    self.num_classes = num_classes\n","    hidden_out = num_classes\n","    self.fc_num = fc_num\n","\n","    self.object_mlp=torch.nn.Sequential(\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden),\n","        ReLU(),\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden_out)\n","    )\n","  \n","\n","    # BN initialization.\n","    for m in self.modules():\n","        if isinstance(m, (torch.nn.BatchNorm1d)):\n","            torch.nn.init.constant_(m.weight, 1)\n","            torch.nn.init.constant_(m.bias,0.0001)\n","\n","  def forward(self,causal,edge_index,causal_edge_weight,batch,eval_random=True):\n","\n","    \n","    xo = F.relu(self.objects_convs(self.bno(causal), edge_index, causal_edge_weight))\n","\n","    #xc_logis = self.context_readout_layer(xc)\n","    #xo_logis = self.objects_readout_layer(xo)\n","    #xco_logis = self.random_readout_layer(xc, xo, eval_random=eval_random)\n","\n","    #return xc_logis, xo_logis, xco_logis\n","    return self.objects_readout_layer(xo,batch)\n","\n","\n","\n","\n","  def objects_readout_layer(self,x,batch):\n","      xo = self.global_pool(x, batch)\n","      x_logis = F.log_softmax(self.object_mlp(xo), dim=-1)\n","      return x_logis\n","\n"]},{"cell_type":"code","source":["class causalpreNO(torch.nn.Module):\n","  def __init__(self, num_features,num_classes,gfn=False,collapse=False,residual=False,\n","                      res_branch=\"BNConvReLU\", \n","                      global_pool=\"sum\", \n","                      dropout=0, \n","                      edge_norm=True):\n","    super(causalpreNO, self).__init__()\n","    num_conv_layers = layers\n","\n","    self.global_pool = global_add_pool\n","\n","    self.with_random = with_random\n"," \n","    GConv = partial(GCNConv, edge_norm=edge_norm, gfn=gfn)\n","\n","    hidden_in = num_features\n","    self.num_classes = num_classes\n","    hidden_out = num_classes\n","    self.fc_num = fc_num\n","    self.bnc = BatchNorm1d(hidden)\n","    self.bno= BatchNorm1d(hidden)\n","    self.context_convs = GConv(hidden, hidden)\n","    self.objects_convs = GConv(hidden, hidden)\n","   \n","    self.context_mlp=torch.nn.Sequential(\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden),\n","        ReLU(),\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden_out)\n","    )\n"," \n","    self.random_readout_mlp=torch.nn.Sequential(\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden),\n","        ReLU(),\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden_out)\n","    )\n","    #else:\n","      #   assert False\n","\n","    # BN initialization.\n","    for m in self.modules():\n","        if isinstance(m, (torch.nn.BatchNorm1d)):\n","            torch.nn.init.constant_(m.weight, 1)\n","            torch.nn.init.constant_(m.bias, 0.0001)\n","\n","  #def forward(self,causal,noncausal,batch,causal_edge_weight,noncausal_edge_weight,edge_index,eval_random=True):\n","  def forward(self,causal,noncausal,edge_index,causal_edge_weight,noncausal_edge_weight,batch,eval_random=True):\n","\n","    xc = F.relu(self.context_convs(self.bnc(noncausal), edge_index, noncausal_edge_weight))\n","    xo = F.relu(self.objects_convs(self.bno(causal), edge_index, causal_edge_weight))\n","\n","    xc = self.global_pool(xc, batch)\n","    xo = self.global_pool(xo, batch)\n","      \n","    xc_logis = self.context_readout_layer(xc)\n","    #xo_logis = self.objects_readout_layer(xo)\n","    xco_logis = self.random_readout_layer(xc, xo, eval_random=eval_random)\n","\n","    return xc_logis, xco_logis\n","\n","\n","  def context_readout_layer(self, x):\n","      \n","\n","      x_logis = F.log_softmax(self.context_mlp(x), dim=-1)\n","      return x_logis\n","\n","  def random_readout_layer(self, xc, xo, eval_random):\n","\n","      num = xc.shape[0]\n","      l = [i for i in range(num)]\n","      if self.with_random:\n","          if eval_random:\n","              random.shuffle(l)\n","      random_idx = torch.tensor(l)\n","      #if self.args.cat_or_add == \"cat\":\n","      #    x = torch.cat((xc[random_idx], xo), dim=1)\n","      #else:\n","      x = xc[random_idx] + xo\n","\n","      x_logis = F.log_softmax(self.random_readout_mlp(x), dim=-1)\n","      return x_logis\n"],"metadata":{"id":"epKUsoafv8Wv","executionInfo":{"status":"ok","timestamp":1684722998892,"user_tz":-720,"elapsed":23,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class causalpreNO(torch.nn.Module):\n","  def __init__(self, num_features,num_classes,gfn=False,collapse=False,residual=False,\n","                      res_branch=\"BNConvReLU\", \n","                      global_pool=\"sum\", \n","                      dropout=0, \n","                      edge_norm=True):\n","    super(causalpreNO, self).__init__()\n","    num_conv_layers = layers\n","\n","    self.global_pool = global_add_pool\n","\n","    self.with_random = with_random\n"," \n","    GConv = partial(GCNConv, edge_norm=edge_norm, gfn=gfn)\n","\n","    hidden_in = num_features\n","    self.num_classes = num_classes\n","    hidden_out = num_classes\n","    self.fc_num = fc_num\n","    self.bnc = BatchNorm1d(hidden)\n","    self.bno= BatchNorm1d(hidden)\n","    self.context_convs = GConv(hidden, hidden)\n","    self.objects_convs = GConv(hidden, hidden)\n","   \n","    self.context_mlp=torch.nn.Sequential(\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden),\n","        ReLU(),\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden_out)\n","    )\n"," \n","    self.random_readout_mlp=torch.nn.Sequential(\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden),\n","        ReLU(),\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden_out)\n","    )\n","    #else:\n","      #   assert False\n","\n","    # BN initialization.\n","    for m in self.modules():\n","        if isinstance(m, (torch.nn.BatchNorm1d)):\n","            torch.nn.init.constant_(m.weight, 1)\n","            torch.nn.init.constant_(m.bias, 0.0001)\n","\n","  #def forward(self,causal,noncausal,batch,causal_edge_weight,noncausal_edge_weight,edge_index,eval_random=True):\n","  def forward(self,causal,noncausal,edge_index,causal_edge_weight,noncausal_edge_weight,batch,eval_random=True):\n","\n","    xc = F.relu(self.context_convs(self.bnc(noncausal), edge_index, noncausal_edge_weight))\n","    xo = F.relu(self.objects_convs(self.bno(causal), edge_index, causal_edge_weight))\n","\n","    xc = self.global_pool(xc, batch)\n","    xo = self.global_pool(xo, batch)\n","      \n","    xc_logis = self.context_readout_layer(xc)\n","    #xo_logis = self.objects_readout_layer(xo)\n","    xco_logis = self.random_readout_layer(xc, xo, eval_random=eval_random)\n","\n","    return xc_logis, xco_logis\n","\n","\n","  def context_readout_layer(self, x):\n","      \n","\n","      x_logis = F.log_softmax(self.context_mlp(x), dim=-1)\n","      return x_logis\n","\n","  def random_readout_layer(self, xc, xo, eval_random):\n","\n","      num = xc.shape[0]\n","      l = [i for i in range(num)]\n","      if self.with_random:\n","          if eval_random:\n","              random.shuffle(l)\n","      random_idx = torch.tensor(l)\n","      #if self.args.cat_or_add == \"cat\":\n","      #    x = torch.cat((xc[random_idx], xo), dim=1)\n","      #else:\n","      x = xc[random_idx] + xo\n","\n","      x_logis = F.log_softmax(self.random_readout_mlp(x), dim=-1)\n","      return x_logis\n"],"metadata":{"executionInfo":{"status":"ok","timestamp":1684722998892,"user_tz":-720,"elapsed":22,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}},"id":"yeGyMs6CXHzH"},"execution_count":9,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"id":"64_EwVWmnugN","executionInfo":{"status":"ok","timestamp":1684722998892,"user_tz":-720,"elapsed":21,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}}},"outputs":[],"source":["class CausalGAN(torch.nn.Module):\n","    \"\"\"GCN with BN and residual connection.\"\"\"\n","    def __init__(self, num_features,\n","                       num_classes,\n","                       gfn=False, \n","                       collapse=False, \n","                       residual=False,\n","                       res_branch=\"BNConvReLU\", \n","                       global_pool=\"sum\", \n","                       dropout=0.2, \n","                       edge_norm=True):\n","        super(CausalGAN, self).__init__()\n","        num_conv_layers = layers\n","        #hidden = args.hidden\n","        #self.args = args\n","        hidden=32\n","        head=4\n","        self.global_pool = global_add_pool\n","        self.dropout = dropout\n","        self.with_random = with_random\n","        self.without_node_attention = False\n","        self.without_edge_attention = False\n","        GConv = partial(GCNConv, edge_norm=edge_norm, gfn=gfn)\n","\n","        hidden_in = num_features\n","        self.num_classes = num_classes\n","        hidden_out = num_classes\n","        self.fc_num = fc_num\n","        self.bn_feat = BatchNorm1d(hidden_in)\n","        self.conv_feat = GCNConv(hidden_in, hidden, gfn=True) # linear transform\n","        self.bns_conv = torch.nn.ModuleList()\n","        self.convs = torch.nn.ModuleList()\n","\n","        for i in range(num_conv_layers):\n","            self.bns_conv.append(BatchNorm1d(hidden))\n","            self.convs.append(GATConv(hidden, int(hidden / head), heads=head, dropout=dropout))\n","\n","        self.edge_att_mlp = nn.Linear(hidden * 2, 2)\n","        self.node_att_mlp = nn.Linear(hidden, 2)\n","        self.bnc = BatchNorm1d(hidden)\n","        self.bno= BatchNorm1d(hidden)\n","        self.context_convs = GConv(hidden, hidden)\n","        self.objects_convs = GConv(hidden, hidden)\n","        self.relu = nn.ReLU(inplace=False)\n","\n","        for m in self.modules():\n","            if isinstance(m, (torch.nn.BatchNorm1d)):\n","                torch.nn.init.constant_(m.weight, 1)\n","                torch.nn.init.constant_(m.bias, 0.0001)\n","\n","    def forward(self, data, eval_random=True):\n","\n","        x = data.x if data.x is not None else data.feat\n","        edge_index,edge_attr,batch = data.edge_index,data.edge_attr,data.batch\n","        row, col = edge_index\n","        x = self.bn_feat(x)\n","        x = self.relu(self.conv_feat(x, edge_index,edge_weight=edge_attr))\n","        \n","        for i, conv in enumerate(self.convs):\n","            x = self.bns_conv[i](x)\n","            x = self.relu(conv(x, edge_index))\n","        \n","        edge_rep = torch.cat([x[row], x[col]], dim=-1)\n","\n","        if self.without_edge_attention:\n","            edge_att = 0.5 * torch.ones(edge_rep.shape[0], 2).to(device)\n","        else:\n","            edge_att = F.softmax(self.edge_att_mlp(edge_rep), dim=-1)\n","        edge_weight_c = edge_att[:, 0]\n","        edge_weight_o = edge_att[:, 1]\n","\n","        if self.without_node_attention:\n","            node_att = 0.5 * torch.ones(x.shape[0], 2).to(device)\n","        else:\n","            node_att = F.softmax(self.node_att_mlp(x), dim=-1)\n","        xc = node_att[:, 0].view(-1, 1) * x\n","        xo = node_att[:, 1].view(-1, 1) * x\n","\n","        #print(edge_weight_o)\n","        #xc = F.relu(self.context_convs(self.bnc(xc), edge_index, edge_weight_c))\n","        #xo = F.relu(self.objects_convs(self.bno(xo), edge_index, edge_weight_o))\n","        #node_of_graph=xo.shape[0]\n","        #causal_part,causal_node=split_graph(graph_x=xo,node_of_graph=node_of_graph)\n","        #noncausal_part,noncausal_node=split_graph(graph_x=xc,node_of_graph=node_of_graph,type_of_graph=False)\n","\n","        #xc = self.global_pool(xc, batch)\n","        #xo = self.global_pool(xo, batch)\n","        \n","        #xc_logis = self.context_readout_layer(xc)\n","        #xo_logis = self.objects_readout_layer(xo)\n","        #xco_logis = self.random_readout_layer(xc, xo, eval_random=eval_random)\n","\n","      # return (xo,edge_index,edge_weight_o),(xc,edge_index,edge_weight_c),batch\n","        return (xo,edge_weight_o),(xc,edge_weight_c),edge_index,batch\n","        #return (causal_attention,causal_part,causal_node),(noncausal_attention,noncausal_part,noncausal_node),batch\n","        #return (xo,causal_part,causal_node),(xc,noncausal_part,noncausal_node),batch\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.datasets import Planetoid\n","from torch_geometric.nn import GCNConv\n","from torch_geometric.data import DataLoader\n","\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n"],"metadata":{"id":"85yBn66PB_QG","executionInfo":{"status":"ok","timestamp":1684722998892,"user_tz":-720,"elapsed":21,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["train_loader = DataLoader(training_final, batch_size=128, shuffle=True)\n","val_loader = DataLoader(valing_final, batch_size=128, shuffle=False)\n","#t_load=[]\n","#for i in train_loader:\n","#  t_load.append(i)\n","#  if(len(t_load)==10000):\n","#    break\n"],"metadata":{"id":"ewtiaFv7H4AC","executionInfo":{"status":"ok","timestamp":1684722998893,"user_tz":-720,"elapsed":21,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"853d2a5f-edff-4206-add8-e0ddc1970d6b"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n","  warnings.warn(out)\n"]}]},{"cell_type":"code","source":["number_of_class=10\n","causal_model2 = CausalGAN(7,number_of_class).to(device)\n","predictno_model2=causalpreNO(7,number_of_class).to(device)\n","predictco_model2=causalpreCO(7,number_of_class).to(device)\n","model_optimizer2 = torch.optim.Adam( \n","            list(causal_model2.parameters()) +\n","            list(predictno_model2.parameters())+list(predictco_model2.parameters()),\n","            lr=0.001)\n","Epo=500\n","lr_scheduler = CosineAnnealingLR(model_optimizer2, T_max=Epo, eta_min=1e-6, last_epoch=-1, verbose=False)"],"metadata":{"id":"oqj2r-DqOxaT","executionInfo":{"status":"ok","timestamp":1684722998893,"user_tz":-720,"elapsed":20,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NvidGrhFBdK4"},"outputs":[],"source":["import time\n","import json\n","\n","loss_value=[]\n","loss_value_valation=[]\n","c=0.5\n","o=1\n","co=0.5\n","def num_graphs(data):\n","  if data.batch is not None:\n","      return data.num_graphs\n","  else:\n","      return data.x.size(0)\n","from tqdm import tqdm\n","Epo=500\n","for epoch in range(Epo):\n","  #model.train()\n","  causal_model2.train()\n","  predictno_model2.train()\n","  predictco_model2.train()\n","  total_loss = 0\n","  total_loss_c = 0\n","  total_loss_o = 0\n","  total_loss_co = 0\n","  correct_o = 0\n","  nb=0\n","  print(f\"-----training-------{epoch}\")\n","  loop = tqdm(enumerate(train_loader),total=len(train_loader))\n","  for it, data in loop:\n","#  for it, data in enumerate(train_loader):\n","      nb+=1\n","      model_optimizer2.zero_grad()\n","      data = data.to(device)\n","      \n","      one_hot_target = data.y.view(-1)\n","      #causal,noncausal,batch=causal_model(data)\n","      (xo,edge_weight_o),(xc,edge_weight_c),edge_index,batch=causal_model2(data)\n","      #c_logs, o_logs, co_logs = predict_model(causal=causal,noncausal=noncausal,batch=batch,causal_edge_weight=edge_weight_causal,noncausal_edge_weight=edge_weight_noncausal,edge_index=edge_index)\n","      c_logs,co_logs = predictno_model2(causal=xo,noncausal=xc,edge_index=edge_index,causal_edge_weight=edge_weight_o,noncausal_edge_weight=edge_weight_c,batch=batch)\n","      o_logs=predictco_model2(causal=xo,edge_index=edge_index,causal_edge_weight=edge_weight_o,batch=batch)\n","      uniform_target = torch.ones_like(c_logs, dtype=torch.float).to(device)/number_of_class\n","     # sim=F.cosine_similarity(causal_attention,noncausal_attention).mean()\n","      #print(o_logs)\n","      #print(type(o_logs))\n","      c_loss = F.kl_div(c_logs, uniform_target,reduction='batchmean')\n","      #print(f'causal loss{c_loss}')\n","      o_loss = F.nll_loss(o_logs,one_hot_target)\n","      #print(f'noncausal loss{o_loss}')\n","      co_loss = F.nll_loss(co_logs,one_hot_target)\n","      #print(f'com causal loss{co_loss}')\n","      loss = c * c_loss + o * o_loss + co * co_loss\n","\n","      pred_o = o_logs.max(1)[1]\n","      correct_o += pred_o.eq(data.y.view(-1)).sum().item()\n","      loss.backward()\n","      total_loss += loss.item() #* num_graphs(data)\n","      total_loss_c += c_loss.item() #* num_graphs(data)\n","      total_loss_o += o_loss.item() #* num_graphs(data)\n","      total_loss_co += co_loss.item()# * num_graphs(data)\n","      model_optimizer2.step()\n","      loop.set_description(f\"Epoch [{epoch}/{Epo}]\")\n","      loop.set_postfix(loss = loss.item())\n","\n","  #num = len(train_loader.dataset)\n","  total_loss = total_loss / nb\n","  total_loss_c = total_loss_c / nb\n","  print(f'number of {epoch} with total loss:{total_loss}')\n","  loss_value.append(total_loss)\n","  total_loss_o = total_loss_o / nb\n","  total_loss_co = total_loss_co / nb\n","  correct_o = correct_o / nb\n","  lr_scheduler.step()\n","  with torch.no_grad():\n","    correct=0\n","    correct_c=0\n","    correct_o=0\n","    print(f\"------valation---------{epoch}\")\n","    causal_model2.eval()\n","    predictno_model2.eval()\n","    predictco_model2.eval()\n","    for data in val_loader:\n","      (xo,edge_weight_o),(xc,edge_weight_c),edge_index,batch=causal_model2(data)\n","      #c_logs, o_logs, co_logs = predict_model(causal=causal,noncausal=noncausal,batch=batch,causal_edge_weight=edge_weight_causal,noncausal_edge_weight=edge_weight_noncausal,edge_index=edge_index)\n","      c_logs,co_logs = predictno_model2(causal=xo,noncausal=xc,edge_index=edge_index,causal_edge_weight=edge_weight_o,noncausal_edge_weight=edge_weight_c,batch=batch)\n","      o_logs=predictco_model2(causal=xo,edge_index=edge_index,causal_edge_weight=edge_weight_o,batch=batch)\n","      pred = co_logs.max(1)[1]\n","      pred_c = c_logs.max(1)[1] \n","      pred_o = o_logs.max(1)[1]\n","      correct += pred.eq(data.y.view(-1)).sum().item()\n","      correct_c += pred_c.eq(data.y.view(-1)).sum().item()\n","      correct_o += pred_o.eq(data.y.view(-1)).sum().item()\n","    acc_co = correct / len(val_loader.dataset)\n","    acc_c = correct_c / len(val_loader.dataset)\n","    acc_o = correct_o / len(val_loader.dataset)\n","    print(f\"causal val accuracy:{acc_co}\")\n","    loss_value_valation.append(acc_co)\n","    dictionary={\"number of epoch\":epoch,\n","                \"training loss list\":loss_value,\n","                \"valation accuracy list\":loss_value_valation}\n"," \n","    # Serializing json\n","    json_object = json.dumps(dictionary,indent=3)\n","    \n","    # Writing to sample.json\n","    with open(\"/content/drive/MyDrive/running_cal_mnist/number_tl_va_e095.json\", \"w\") as outfile:\n","        outfile.write(json_object)\n","\n","    #torch.save(causal_model.state_dict(), '/content/drive/MyDrive/Colab_Notebooks/430cau_my6000.pt')\n","    #torch.save(predictco_model.state_dict(), '/content/drive/MyDrive/Colab_Notebooks/430caupred_my6000.pt')\n","    #torch.save(predictno_model.state_dict(), '/content/drive/MyDrive/Colab_Notebooks/430noncaupred_my6000.pt')\n","    #torch.save(model_heter.state_dict(), '/content/drive/MyDrive/Colab_Notebooks/430heter6000.pt')\n","    torch.save({\n","            'causal_model.state_dic': causal_model2.state_dict(),\n","            'predictco_model_state_dict()': predictco_model2.state_dict(),\n","            'predictno_model_state_dict()': predictno_model2.state_dict(),\n","            'opt':model_optimizer2.state_dict()\n","            }, '/content/drive/MyDrive/running_cal_mnist/allmodel_cal_095.pt')\n","    if(epoch>50):\n","      check=abs(acc_o-loss_value_valation[len(loss_value_valation)-20])/20\n","      if(check<=0.001):\n","        break\n","\n","\n","  #model_optimizer.zero_grad()\n","  #total_loss.backward()\n","  #model_optimizer.step()\n","test_loader = DataLoader(testing_final, batch_size=1, shuffle=False)\n","\n","correct=0\n","correct_c=0\n","correct_o=0\n","print(f\"------test---------{0000000}\")\n","causal_model2.eval()\n","predictno_model2.eval()\n","predictco_model2.eval()\n","for data in test_loader:\n","  (xo,edge_weight_o),(xc,edge_weight_c),edge_index,batch=causal_model2(data)\n","  #c_logs, o_logs, co_logs = predict_model(causal=causal,noncausal=noncausal,batch=batch,causal_edge_weight=edge_weight_causal,noncausal_edge_weight=edge_weight_noncausal,edge_index=edge_index)\n","  c_logs,co_logs = predictno_model2(causal=xo,noncausal=xc,edge_index=edge_index,causal_edge_weight=edge_weight_o,noncausal_edge_weight=edge_weight_c,batch=batch)\n","  o_logs=predictco_model2(causal=xo,edge_index=edge_index,causal_edge_weight=edge_weight_o,batch=batch)\n","  pred = co_logs.max(1)[1]\n","  pred_c = c_logs.max(1)[1] \n","  pred_o = o_logs.max(1)[1]\n","  correct += pred.eq(data.y.view(-1)).sum().item()\n","  correct_c += pred_c.eq(data.y.view(-1)).sum().item()\n","  correct_o += pred_o.eq(data.y.view(-1)).sum().item()\n","acc_co = correct / len(test_loader.dataset)\n","acc_c = correct_c / len(test_loader.dataset)\n","acc_o = correct_o / len(test_loader.dataset)\n","print(f\"com causal test accuracy:{acc_co}\")\n","print(f\"causal test accuracy:{acc_o}\")\n","dictionary={\"number of epoch\":epoch,\n","            \"training loss list\":loss_value,\n","            \"valation accuracy list\":loss_value_valation,\n","            \"test accuracy value\":acc_co}\n"," \n","# Serializing json\n","json_object = json.dumps(dictionary,indent=4)\n","\n","# Writing to sample.json\n","with open(\"/content/drive/MyDrive/running_cal_mnist/number_tl_va_e095.json\", \"w\") as outfile:\n","    outfile.write(json_object)"]},{"cell_type":"code","source":["aaa"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":169},"id":"6nAmKSAnoxYC","executionInfo":{"status":"error","timestamp":1684725108537,"user_tz":-720,"elapsed":21,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}},"outputId":"7e9f1cca-dd0d-4933-cf7f-d4a55a3f9207"},"execution_count":15,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-972a1a11f199>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maaa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'aaa' is not defined"]}]},{"cell_type":"code","source":["checkpoint = torch.load('/content/drive/MyDrive/running_cal_mnist/allmodel_cal_095.pt')\n","causal_model2.load_state_dict(checkpoint['causal_model.state_dic'])\n","predictco_model2.load_state_dict(checkpoint['predictco_model_state_dict()'])\n","predictno_model2.load_state_dict(checkpoint['predictno_model_state_dict()'])"],"metadata":{"id":"XjZxV7g0uBqp","executionInfo":{"status":"aborted","timestamp":1684725108538,"user_tz":-720,"elapsed":15,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_loader = DataLoader(testing_final, batch_size=1, shuffle=False)\n","\n","correct=0\n","correct_c=0\n","correct_o=0\n","print(f\"------test---------{0000000}\")\n","causal_model2.eval()\n","predictno_model2.eval()\n","predictco_model2.eval()\n","for data in test_loader:\n","  (xo,edge_weight_o),(xc,edge_weight_c),edge_index,batch=causal_model2(data)\n","  #c_logs, o_logs, co_logs = predict_model(causal=causal,noncausal=noncausal,batch=batch,causal_edge_weight=edge_weight_causal,noncausal_edge_weight=edge_weight_noncausal,edge_index=edge_index)\n","  c_logs,co_logs = predictno_model2(causal=xo,noncausal=xc,edge_index=edge_index,causal_edge_weight=edge_weight_o,noncausal_edge_weight=edge_weight_c,batch=batch)\n","  o_logs=predictco_model2(causal=xo,edge_index=edge_index,causal_edge_weight=edge_weight_o,batch=batch)\n","  pred = co_logs.max(1)[1]\n","  pred_c = c_logs.max(1)[1] \n","  pred_o = o_logs.max(1)[1]\n","  correct += pred.eq(data.y.view(-1)).sum().item()\n","  correct_c += pred_c.eq(data.y.view(-1)).sum().item()\n","  correct_o += pred_o.eq(data.y.view(-1)).sum().item()\n","acc_co = correct / len(test_loader.dataset)\n","acc_c = correct_c / len(test_loader.dataset)\n","acc_o = correct_o / len(test_loader.dataset)\n","print(f\"com causal test accuracy:{acc_co}\")\n","print(f\"causal test accuracy:{acc_o}\")\n","#dictionary={\"number of epoch\":epoch,\n","#            \"training loss list\":loss_value,\n","#            \"valation accuracy list\":loss_value_valation,\n","#            \"test accuracy value\":acc_co}\n"," \n","# Serializing json\n","#json_object = json.dumps(dictionary,indent=4)\n","\n","# Writing to sample.json\n","#with open(\"/content/drive/MyDrive/running_cal_mnist/number_tl_va_e095.json\", \"w\") as outfile:\n","#    outfile.write(json_object)\n"],"metadata":{"id":"tSV_5LT74ACj","executionInfo":{"status":"aborted","timestamp":1684725108539,"user_tz":-720,"elapsed":16,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}}},"execution_count":null,"outputs":[]}]}