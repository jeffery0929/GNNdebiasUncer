{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1rsHmUS7i0Mf1-GRGyCKKvGYwTYg6MfHZ","authorship_tag":"ABX9TyP2rG99pkPBm8RMObBVpt1a"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xVEXjGk1WGRp","executionInfo":{"status":"ok","timestamp":1687075792470,"user_tz":-720,"elapsed":35102,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}},"outputId":"8875d9d1-7e2c-4f8c-a00a-45f945760b23"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.0.1+cu118\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\n","Collecting pyg_lib\n","  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/pyg_lib-0.2.0%2Bpt20cpu-cp310-cp310-linux_x86_64.whl (627 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m627.0/627.0 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_scatter\n","  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_scatter-2.1.1%2Bpt20cpu-cp310-cp310-linux_x86_64.whl (504 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.1/504.1 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_sparse\n","  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_sparse-0.6.17%2Bpt20cpu-cp310-cp310-linux_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_cluster\n","  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_cluster-1.6.1%2Bpt20cpu-cp310-cp310-linux_x86_64.whl (732 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m732.3/732.3 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_spline_conv\n","  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_spline_conv-1.2.2%2Bpt20cpu-cp310-cp310-linux_x86_64.whl (205 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.7/205.7 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.10.1)\n","Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch_sparse) (1.22.4)\n","Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n","Successfully installed pyg_lib-0.2.0+pt20cpu torch_cluster-1.6.1+pt20cpu torch_scatter-2.1.1+pt20cpu torch_sparse-0.6.17+pt20cpu torch_spline_conv-1.2.2+pt20cpu\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement warnings (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for warnings\u001b[0m\u001b[31m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting dgl\n","  Downloading dgl-1.1.0-cp310-cp310-manylinux1_x86_64.whl (5.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.22.4)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.10.1)\n","Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.65.0)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4)\n","Installing collected packages: dgl\n","Successfully installed dgl-1.1.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting texttable\n","  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n","Installing collected packages: texttable\n","Successfully installed texttable-1.6.7\n","cpu\n"]}],"source":["import os\n","import torch\n","os.environ['TORCH'] = torch.__version__\n","print(torch.__version__)\n","\n","#!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n","#!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n","!pip install warnings\n","!pip install dgl\n","!pip install texttable\n","\n","if torch.cuda.is_available():\n","  device = torch.device(\"cuda\")\n","else:\n","  device = torch.device(\"cpu\")\n","print(device)"]},{"cell_type":"code","source":["#!unzip '/content/drive/MyDrive/unseen_cmist/unseen_test.zip'"],"metadata":{"id":"V_LafOUCh-V8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dq-Zd6aQWRvb","executionInfo":{"status":"ok","timestamp":1684415178103,"user_tz":-720,"elapsed":11,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}},"outputId":"26d32682-0c0e-4722-a2b7-7d0d3600567a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":[],"metadata":{"id":"cGqoLa84WV7k"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"LtHqVCYeNmmv","executionInfo":{"status":"ok","timestamp":1687075829829,"user_tz":-720,"elapsed":37364,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"288f129c-d11a-4054-cd3c-037267cd05fb"},"outputs":[{"output_type":"stream","name":"stderr","text":["DGL backend not selected or invalid.  Assuming PyTorch for now.\n"]},{"output_type":"stream","name":"stdout","text":["Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"]}],"source":["import random\n","from torchvision import transforms, datasets\n","\n","import os\n","import pickle\n","from scipy.spatial.distance import cdist\n","from scipy import ndimage\n","import numpy as np\n","\n","import dgl\n","import torch\n","import time\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","import matplotlib\n","def sigma(dists, kth=8):\n","    # Get k-nearest neighbors for each node\n","    knns = np.partition(dists, kth, axis=-1)[:, kth::-1]\n","\n","    # Compute sigma and reshape\n","    sigma = knns.sum(axis=1).reshape((knns.shape[0], 1))/kth\n","    return sigma + 1e-8 # adding epsilon to avoid zero value of sigma\n","\n","def compute_adjacency_matrix_images(coord, feat, use_feat=False, kth=8):\n","    coord = coord.reshape(-1, 2)\n","    # Compute coordinate distance\n","    c_dist = cdist(coord, coord)\n","\n","    if use_feat:\n","        # Compute feature distance\n","        f_dist = cdist(feat, feat)\n","        # Compute adjacency\n","        A = np.exp(- (c_dist/sigma(c_dist))**2 - (f_dist/sigma(f_dist))**2 )\n","    else:\n","        A = np.exp(- (c_dist/sigma(c_dist))**2)\n","\n","    # Convert to symmetric matrix\n","    A = 0.5 * A * A.T\n","    A[np.diag_indices_from(A)] = 0\n","    return A\n","\n","def compute_edges_list(A, kth=8+1):\n","    # Get k-similar neighbor indices for each node\n","    if 1==1:\n","        num_nodes = A.shape[0]\n","        new_kth = num_nodes - kth\n","        knns = np.argpartition(A, new_kth-1, axis=-1)[:, new_kth:-1]\n","        knns_d = np.partition(A, new_kth-1, axis=-1)[:, new_kth:-1]\n","    else:\n","        knns = np.argpartition(A, kth, axis=-1)[:, kth::-1]\n","        knns_d = np.partition(A, kth, axis=-1)[:, kth::-1]\n","    return knns, knns_d\n","class newCIFARSuperPix(torch.utils.data.Dataset):\n","    def __init__(self,\n","                 data_dir,\n","                 use_mean_px=True,\n","                 use_coord=True,\n","                 use_feat_for_graph_construct=False,):\n","\n","        #self.split = split\n","        #self.is_test = split.lower() in ['test', 'val']\n","        with open(data_dir, 'rb') as f:\n","            self.labels, self.sp_data = pickle.load(f)\n","\n","        self.use_mean_px = use_mean_px\n","        self.use_feat_for_graph = use_feat_for_graph_construct\n","        self.use_coord = use_coord\n","        self.n_samples = len(self.labels)\n","        self.img_size = 32\n","\n","    def precompute_graph_images(self):\n","        #print('precompute all data for the %s set...' % self.split.upper())\n","        self.Adj_matrices, self.node_features, self.edges_lists = [], [], []\n","        for index, sample in enumerate(self.sp_data):\n","            mean_px, coord = sample[:2]\n","            coord = coord / self.img_size\n","            A = compute_adjacency_matrix_images(coord, mean_px, use_feat=self.use_feat_for_graph)\n","            edges_list, _ = compute_edges_list(A)\n","            N_nodes = A.shape[0]\n","\n","            x = None\n","            if self.use_mean_px:\n","                x = mean_px.reshape(N_nodes, -1)\n","            if self.use_coord:\n","                coord = coord.reshape(N_nodes, 2)\n","                if self.use_mean_px:\n","                    x = np.concatenate((x, coord), axis=1)\n","                else:\n","                    x = coord\n","            if x is None:\n","                x = np.ones(N_nodes, 1)  # dummy features\n","\n","            self.node_features.append(x)\n","            self.Adj_matrices.append(A)\n","            self.edges_lists.append(edges_list)\n","\n","    def __len__(self):\n","        return self.n_samples\n","\n","    def __getitem__(self, index):\n","        g = dgl.DGLGraph()\n","        g.add_nodes(self.node_features[index].shape[0])\n","        g.ndata['feat'] = torch.Tensor(self.node_features[index])\n","        for src, dsts in enumerate(self.edges_lists[index]):\n","            g.add_edges(src, dsts[dsts!=src])\n","\n","        return g, self.labels[index]\n","\n","use_feat_for_graph_construct = False\n","tt = time.time()\n","data_with_feat_knn = newCIFARSuperPix(\"/content/drive/MyDrive/CMINST_data/CMNIST09_60000_75sp_train.pkl\",use_feat_for_graph_construct=use_feat_for_graph_construct)\n","\n","data_with_feat_knn.precompute_graph_images()\n","training_data = np.load('/content/drive/MyDrive/CMINST_data/colorMNIST09_60000_data.npy')\n","training_label=np.load('/content/drive/MyDrive/CMINST_data/colorMNIST09_60000_label.npy')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"G0eqa0s54oKO","executionInfo":{"status":"ok","timestamp":1687075832143,"user_tz":-720,"elapsed":2319,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}}},"outputs":[],"source":["import numpy as np\n","import os.path as osp\n","import pickle\n","import torch\n","import torch.utils\n","import torch.utils.data\n","import torch.nn.functional as F\n","from scipy.spatial.distance import cdist\n","from torch_geometric.utils import dense_to_sparse\n","from torch_geometric.data import InMemoryDataset,Data\n","#/content/drive/MyDrive/Colab_Notebooks/mnist08_83_75sp_train.pkl\n","def compute_adjacency_matrix_images(coord, sigma=0.1):\n","    coord = coord.reshape(-1, 2)\n","    dist = cdist(coord, coord)\n","    A = np.exp(- dist / (sigma * np.pi) ** 2)\n","    A[np.diag_indices_from(A)] = 0\n","    return A\n","\n","\n","def list_to_torch(data):\n","    for i in range(len(data)):\n","        if data[i] is None:\n","            continue\n","        elif isinstance(data[i], np.ndarray):\n","            if data[i].dtype == np.bool:\n","                data[i] = data[i].astype(np.float32)\n","            data[i] = torch.from_numpy(data[i]).float()\n","        elif isinstance(data[i], list):\n","            data[i] = list_to_torch(data[i])\n","    return data\n","def process(data_file):\n","  use_mean_px=True\n","  use_coord=True\n","  node_gt_att_threshold=0\n","  transform=None\n","  pre_transform=None\n","  pre_filter=None\n","\n","  #data_file ='/content/drive/MyDrive/Colab_Notebooks/colorMNIST05_2000_75sp_train.pkl'\n","\n","  with open(osp.join(data_file), 'rb') as f:\n","      labels,sp_data = pickle.load(f)\n","\n","  #use_mean_px = self.use_mean_px\n","  #self.use_coord = self.use_coord\n","  n_samples = len(labels)\n","  img_size = 32\n","  #node_gt_att_threshold = self.node_gt_att_threshold\n","\n","  edge_indices,xs,edge_attrs,node_gt_atts,edge_gt_atts = [], [], [], [], []\n","  data_list = []\n","  for index, sample in enumerate(sp_data):\n","      mean_px, coord = sample[:2]\n","      coord = coord / img_size\n","      A = compute_adjacency_matrix_images(coord)\n","      N_nodes = A.shape[0]\n","\n","      A = torch.FloatTensor((A > 0.1) * A)\n","      edge_index, edge_attr = dense_to_sparse(A)\n","\n","      x = None\n","      if use_mean_px:\n","          x = mean_px.reshape(N_nodes, -1)\n","      if use_coord:\n","          coord = coord.reshape(N_nodes, 2)\n","          if use_mean_px:\n","              x = np.concatenate((x, coord), axis=1)\n","          else:\n","              x = coord\n","      if x is None:\n","          x = np.ones(N_nodes, 1)  # dummy features\n","\n","      # replicate features to make it possible to test on colored images\n","      x = np.pad(x, ((0, 0), (2, 0)), 'edge')\n","      if node_gt_att_threshold == 0:\n","          node_gt_att = (mean_px > 0).astype(np.float32)\n","      else:\n","          node_gt_att = mean_px.copy()\n","          node_gt_att[node_gt_att < node_gt_att_threshold] = 0\n","\n","      node_gt_att = torch.LongTensor(node_gt_att).view(-1)\n","      row, col = edge_index\n","      edge_gt_att = torch.LongTensor(node_gt_att[row] * node_gt_att[col]).view(-1)\n","\n","      data_list.append(\n","          Data(\n","              x=torch.tensor(x),\n","              y=torch.LongTensor([labels[index]]),\n","              edge_index=edge_index,\n","              edge_attr=edge_attr,\n","              node_gt_att=node_gt_att,\n","              edge_gt_att=edge_gt_att\n","\n","          )\n","      )\n","\n","  #torch.save(InMemoryDataset.collate(data_list), '/content/drive/MyDrive/Colab_Notebooks/colorMINST05_2000.pt')\n","  return data_list"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"JrQnfDYkxdR7","executionInfo":{"status":"ok","timestamp":1687075853057,"user_tz":-720,"elapsed":20917,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}}},"outputs":[],"source":["train_dir='/content/drive/MyDrive/CMINST_data/CMNIST09_10000_75sp_train.pkl'\n","val_dir='/content/drive/MyDrive/CMINST_data/CMNIST5000_75sp_val.pkl'\n","test_dir='/content/drive/MyDrive/CMINST_data/CMNIST10000_75sp_test.pkl'\n","training_final=process(data_file=train_dir)\n","valing_final=process(data_file=val_dir)\n","testing_final=process(data_file=test_dir)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Q2RkYPa8k9nE","executionInfo":{"status":"ok","timestamp":1687075853057,"user_tz":-720,"elapsed":22,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}}},"outputs":[],"source":["import torch\n","from torch.nn import Parameter\n","from torch_scatter import scatter_add\n","from torch_geometric.nn.conv import MessagePassing\n","from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n","from torch_geometric.nn.inits import glorot, zeros\n","import pdb\n","def mask_graph(graph_x,select_node):\n","  mask_value=np.array([0.0001]*32)\n","  result=np.array([t.detach().numpy() for t in graph_x])\n","  for i in range(graph_x.shape[0]):\n","    if(i in select_node):\n","      continue\n","    result[i]=mask_value\n","  return torch.tensor(result)\n","\n","\n","\n","\n","\n","def split_graph(graph_x,node_of_graph,type_of_graph=True):\n","  if(type_of_graph==True):\n","    select_node_number=int(node_of_graph/3)\n","    select_node=torch.topk(graph_x.mean(axis=1),select_node_number)[1]\n","    #print(select_node)\n","    return mask_graph(graph_x,select_node),select_node\n","  else:\n","    select_node_number=int(node_of_graph/3*2)\n","    select_node=torch.topk(graph_x.mean(axis=1),select_node_number)[1]\n","    return mask_graph(graph_x,select_node),select_node\n","\n","def uncertainty_mask_gnerate(node_in_graph,number_of_mask):\n","  all_mask=[]\n","  for i in range(number_of_mask):\n","    random_mask=random.sample(node_in_graph.tolist(),int(len(node_in_graph)/3))\n","    all_mask.append(random_mask)\n","  return all_mask\n","def mean(l):\n","  return sum(l)/len(l)\n","class GCNConv(MessagePassing):\n","\n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 improved=False,\n","                 cached=False,\n","                 bias=True,\n","                 edge_norm=True,\n","                 gfn=False):\n","        super(GCNConv, self).__init__('add')\n","\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.improved = improved\n","        self.cached = cached\n","        self.cached_result = None\n","        self.edge_norm = edge_norm\n","        self.gfn = gfn\n","        self.message_mask = None\n","        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n","\n","        if bias:\n","            self.bias = Parameter(torch.Tensor(out_channels))\n","        else:\n","            self.register_parameter('bias', None)\n","\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        glorot(self.weight)\n","        zeros(self.bias)\n","        self.cached_result = None\n","\n","    @staticmethod\n","    def norm(edge_index, num_nodes, edge_weight, improved=False, dtype=None):\n","        if edge_weight is None:\n","            edge_weight = torch.ones((edge_index.size(1), ),\n","                                     dtype=dtype,\n","                                     device=edge_index.device)\n","\n","        edge_weight = edge_weight.view(-1)\n","\n","\n","        assert edge_weight.size(0) == edge_index.size(1)\n","\n","        edge_index, edge_weight = remove_self_loops(edge_index, edge_weight)\n","        edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n","        # Add edge_weight for loop edges.\n","        loop_weight = torch.full((num_nodes, ),\n","                                 1 if not improved else 2,\n","                                 dtype=edge_weight.dtype,\n","                                 device=edge_weight.device)\n","        edge_weight = torch.cat([edge_weight, loop_weight], dim=0)\n","\n","        row, col = edge_index\n","        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n","        deg_inv_sqrt = deg.pow(-0.5)\n","        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n","\n","        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n","\n","    def forward(self, x, edge_index, edge_weight=None):\n","        \"\"\"\"\"\"\n","\n","        x = torch.matmul(x, self.weight)\n","        if self.gfn:\n","            return x\n","\n","        if not self.cached or self.cached_result is None:\n","            if self.edge_norm:\n","                edge_index, norm = GCNConv.norm(\n","                    edge_index,\n","                    x.size(0),\n","                    edge_weight,\n","                    self.improved,\n","                    x.dtype)\n","            else:\n","                norm = None\n","            self.cached_result = edge_index, norm\n","\n","        edge_index, norm = self.cached_result\n","        return self.propagate(edge_index, x=x, norm=norm)\n","\n","    def message(self, x_j, norm):\n","\n","        if self.edge_norm:\n","            return norm.view(-1, 1) * x_j\n","        else:\n","            return x_j\n","\n","    def update(self, aggr_out):\n","        if self.bias is not None:\n","            aggr_out = aggr_out + self.bias\n","        return aggr_out\n","\n","    def __repr__(self):\n","        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n","                                   self.out_channels)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"jCXZmqIT4gvJ","executionInfo":{"status":"ok","timestamp":1687075853058,"user_tz":-720,"elapsed":22,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}}},"outputs":[],"source":["from functools import partial\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import Linear, BatchNorm1d, Sequential, ReLU\n","from torch_geometric.nn import global_mean_pool, global_add_pool, GINConv, GATConv\n","\n","import random\n","import pdb\n","device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n","\n","layers=3\n","with_random=True\n","fc_num=222\n","hidden=32\n","eval_random=False\n","class causalpreCO(torch.nn.Module):\n","  def __init__(self, num_features,num_classes,gfn=False,collapse=False,residual=False,\n","                      res_branch=\"BNConvReLU\",\n","                      global_pool=\"sum\",\n","                      dropout=0,\n","                      edge_norm=True):\n","    super(causalpreCO, self).__init__()\n","    num_conv_layers = layers\n","    self.global_pool = global_add_pool\n","\n","    self.with_random = with_random\n","    GConv = partial(GCNConv, edge_norm=edge_norm, gfn=gfn)\n","    self.bnc = BatchNorm1d(hidden)\n","    self.bno= BatchNorm1d(hidden)\n","    self.context_convs = GConv(hidden, hidden)\n","    self.objects_convs = GConv(hidden, hidden)\n","\n","\n","    hidden_in = num_features\n","    self.num_classes = num_classes\n","    hidden_out = num_classes\n","    self.fc_num = fc_num\n","\n","    self.object_mlp=torch.nn.Sequential(\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden),\n","        ReLU(),\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden_out)\n","    )\n","\n","\n","    # BN initialization.\n","    for m in self.modules():\n","        if isinstance(m, (torch.nn.BatchNorm1d)):\n","            torch.nn.init.constant_(m.weight, 1)\n","            torch.nn.init.constant_(m.bias,0.0001)\n","\n","  def forward(self,causal,edge_index,causal_edge_weight,batch,eval_random=True):\n","\n","\n","    xo = F.relu(self.objects_convs(self.bno(causal), edge_index, causal_edge_weight))\n","\n","    #xc_logis = self.context_readout_layer(xc)\n","    #xo_logis = self.objects_readout_layer(xo)\n","    #xco_logis = self.random_readout_layer(xc, xo, eval_random=eval_random)\n","\n","    #return xc_logis, xo_logis, xco_logis\n","    return self.objects_readout_layer(xo,batch)\n","\n","\n","\n","\n","  def objects_readout_layer(self,x,batch):\n","      xo = self.global_pool(x, batch)\n","      x_logis = F.log_softmax(self.object_mlp(xo), dim=-1)\n","      return x_logis\n","\n"]},{"cell_type":"code","source":["class causalpreNO(torch.nn.Module):\n","  def __init__(self, num_features,num_classes,gfn=False,collapse=False,residual=False,\n","                      res_branch=\"BNConvReLU\",\n","                      global_pool=\"sum\",\n","                      dropout=0,\n","                      edge_norm=True):\n","    super(causalpreNO, self).__init__()\n","    num_conv_layers = layers\n","\n","    self.global_pool = global_add_pool\n","\n","    self.with_random = with_random\n","\n","    GConv = partial(GCNConv, edge_norm=edge_norm, gfn=gfn)\n","\n","    hidden_in = num_features\n","    self.num_classes = num_classes\n","    hidden_out = num_classes\n","    self.fc_num = fc_num\n","    self.bnc = BatchNorm1d(hidden)\n","    self.bno= BatchNorm1d(hidden)\n","    self.context_convs = GConv(hidden, hidden)\n","    self.objects_convs = GConv(hidden, hidden)\n","\n","    self.context_mlp=torch.nn.Sequential(\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden),\n","        ReLU(),\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden_out)\n","    )\n","\n","    self.random_readout_mlp=torch.nn.Sequential(\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden),\n","        ReLU(),\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden_out)\n","    )\n","    #else:\n","      #   assert False\n","\n","    # BN initialization.\n","    for m in self.modules():\n","        if isinstance(m, (torch.nn.BatchNorm1d)):\n","            torch.nn.init.constant_(m.weight, 1)\n","            torch.nn.init.constant_(m.bias, 0.0001)\n","\n","  #def forward(self,causal,noncausal,batch,causal_edge_weight,noncausal_edge_weight,edge_index,eval_random=True):\n","  def forward(self,causal,noncausal,edge_index,causal_edge_weight,noncausal_edge_weight,batch,eval_random=True):\n","\n","    xc = F.relu(self.context_convs(self.bnc(noncausal), edge_index, noncausal_edge_weight))\n","    xo = F.relu(self.objects_convs(self.bno(causal), edge_index, causal_edge_weight))\n","\n","    xc = self.global_pool(xc, batch)\n","    xo = self.global_pool(xo, batch)\n","\n","    xc_logis = self.context_readout_layer(xc)\n","    #xo_logis = self.objects_readout_layer(xo)\n","    xco_logis = self.random_readout_layer(xc, xo, eval_random=eval_random)\n","\n","    return xc_logis, xco_logis\n","\n","\n","  def context_readout_layer(self, x):\n","\n","\n","      x_logis = F.log_softmax(self.context_mlp(x), dim=-1)\n","      return x_logis\n","\n","  def random_readout_layer(self, xc, xo, eval_random):\n","\n","      num = xc.shape[0]\n","      l = [i for i in range(num)]\n","      if self.with_random:\n","          if eval_random:\n","              random.shuffle(l)\n","      random_idx = torch.tensor(l)\n","      #if self.args.cat_or_add == \"cat\":\n","      #    x = torch.cat((xc[random_idx], xo), dim=1)\n","      #else:\n","      x = xc[random_idx] + xo\n","\n","      x_logis = F.log_softmax(self.random_readout_mlp(x), dim=-1)\n","      return x_logis\n"],"metadata":{"id":"epKUsoafv8Wv","executionInfo":{"status":"ok","timestamp":1687075853058,"user_tz":-720,"elapsed":22,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class causalpreNO(torch.nn.Module):\n","  def __init__(self, num_features,num_classes,gfn=False,collapse=False,residual=False,\n","                      res_branch=\"BNConvReLU\",\n","                      global_pool=\"sum\",\n","                      dropout=0,\n","                      edge_norm=True):\n","    super(causalpreNO, self).__init__()\n","    num_conv_layers = layers\n","\n","    self.global_pool = global_add_pool\n","\n","    self.with_random = with_random\n","\n","    GConv = partial(GCNConv, edge_norm=edge_norm, gfn=gfn)\n","\n","    hidden_in = num_features\n","    self.num_classes = num_classes\n","    hidden_out = num_classes\n","    self.fc_num = fc_num\n","    self.bnc = BatchNorm1d(hidden)\n","    self.bno= BatchNorm1d(hidden)\n","    self.context_convs = GConv(hidden, hidden)\n","    self.objects_convs = GConv(hidden, hidden)\n","\n","    self.context_mlp=torch.nn.Sequential(\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden),\n","        ReLU(),\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden_out)\n","    )\n","\n","    self.random_readout_mlp=torch.nn.Sequential(\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden),\n","        ReLU(),\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden_out)\n","    )\n","    #else:\n","      #   assert False\n","\n","    # BN initialization.\n","    for m in self.modules():\n","        if isinstance(m, (torch.nn.BatchNorm1d)):\n","            torch.nn.init.constant_(m.weight, 1)\n","            torch.nn.init.constant_(m.bias, 0.0001)\n","\n","  #def forward(self,causal,noncausal,batch,causal_edge_weight,noncausal_edge_weight,edge_index,eval_random=True):\n","  def forward(self,causal,noncausal,edge_index,causal_edge_weight,noncausal_edge_weight,batch,eval_random=True):\n","\n","    xc = F.relu(self.context_convs(self.bnc(noncausal), edge_index, noncausal_edge_weight))\n","    xo = F.relu(self.objects_convs(self.bno(causal), edge_index, causal_edge_weight))\n","\n","    xc = self.global_pool(xc, batch)\n","    xo = self.global_pool(xo, batch)\n","\n","    xc_logis = self.context_readout_layer(xc)\n","    #xo_logis = self.objects_readout_layer(xo)\n","    xco_logis = self.random_readout_layer(xc, xo, eval_random=eval_random)\n","\n","    return xc_logis, xco_logis\n","\n","\n","  def context_readout_layer(self, x):\n","\n","\n","      x_logis = F.log_softmax(self.context_mlp(x), dim=-1)\n","      return x_logis\n","\n","  def random_readout_layer(self, xc, xo, eval_random):\n","\n","      num = xc.shape[0]\n","      l = [i for i in range(num)]\n","      if self.with_random:\n","          if eval_random:\n","              random.shuffle(l)\n","      random_idx = torch.tensor(l)\n","      #if self.args.cat_or_add == \"cat\":\n","      #    x = torch.cat((xc[random_idx], xo), dim=1)\n","      #else:\n","      x = xc[random_idx] + xo\n","\n","      x_logis = F.log_softmax(self.random_readout_mlp(x), dim=-1)\n","      return x_logis\n"],"metadata":{"id":"yeGyMs6CXHzH","executionInfo":{"status":"ok","timestamp":1687075853058,"user_tz":-720,"elapsed":21,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"id":"64_EwVWmnugN","executionInfo":{"status":"ok","timestamp":1687075853058,"user_tz":-720,"elapsed":21,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}}},"outputs":[],"source":["class CausalGAN(torch.nn.Module):\n","    \"\"\"GCN with BN and residual connection.\"\"\"\n","    def __init__(self, num_features,\n","                       num_classes,\n","                       gfn=False,\n","                       collapse=False,\n","                       residual=False,\n","                       res_branch=\"BNConvReLU\",\n","                       global_pool=\"sum\",\n","                       dropout=0.2,\n","                       edge_norm=True):\n","        super(CausalGAN, self).__init__()\n","        num_conv_layers = layers\n","        #hidden = args.hidden\n","        #self.args = args\n","        hidden=32\n","        head=4\n","        self.global_pool = global_add_pool\n","        self.dropout = dropout\n","        self.with_random = with_random\n","        self.without_node_attention = False\n","        self.without_edge_attention = False\n","        GConv = partial(GCNConv, edge_norm=edge_norm, gfn=gfn)\n","\n","        hidden_in = num_features\n","        self.num_classes = num_classes\n","        hidden_out = num_classes\n","        self.fc_num = fc_num\n","        self.bn_feat = BatchNorm1d(hidden_in)\n","        self.conv_feat = GCNConv(hidden_in, hidden, gfn=True) # linear transform\n","        self.bns_conv = torch.nn.ModuleList()\n","        self.convs = torch.nn.ModuleList()\n","\n","        for i in range(num_conv_layers):\n","            self.bns_conv.append(BatchNorm1d(hidden))\n","            self.convs.append(GATConv(hidden, int(hidden / head), heads=head, dropout=dropout))\n","\n","        self.edge_att_mlp = nn.Linear(hidden * 2, 2)\n","        self.node_att_mlp = nn.Linear(hidden, 2)\n","        self.bnc = BatchNorm1d(hidden)\n","        self.bno= BatchNorm1d(hidden)\n","        self.context_convs = GConv(hidden, hidden)\n","        self.objects_convs = GConv(hidden, hidden)\n","        self.relu = nn.ReLU(inplace=False)\n","\n","        for m in self.modules():\n","            if isinstance(m, (torch.nn.BatchNorm1d)):\n","                torch.nn.init.constant_(m.weight, 1)\n","                torch.nn.init.constant_(m.bias, 0.0001)\n","\n","    def forward(self, data, eval_random=True):\n","\n","        x = data.x if data.x is not None else data.feat\n","        edge_index,edge_attr,batch = data.edge_index,data.edge_attr,data.batch\n","        row, col = edge_index\n","        x = self.bn_feat(x)\n","        x = self.relu(self.conv_feat(x, edge_index,edge_weight=edge_attr))\n","\n","        for i, conv in enumerate(self.convs):\n","            x = self.bns_conv[i](x)\n","            x = self.relu(conv(x, edge_index))\n","\n","        edge_rep = torch.cat([x[row], x[col]], dim=-1)\n","\n","        if self.without_edge_attention:\n","            edge_att = 0.5 * torch.ones(edge_rep.shape[0], 2).to(device)\n","        else:\n","            edge_att = F.softmax(self.edge_att_mlp(edge_rep), dim=-1)\n","        edge_weight_c = edge_att[:, 0]\n","        edge_weight_o = edge_att[:, 1]\n","\n","        if self.without_node_attention:\n","            node_att = 0.5 * torch.ones(x.shape[0], 2).to(device)\n","        else:\n","            node_att = F.softmax(self.node_att_mlp(x), dim=-1)\n","        xc = node_att[:, 0].view(-1, 1) * x\n","        xo = node_att[:, 1].view(-1, 1) * x\n","\n","        #print(edge_weight_o)\n","        #xc = F.relu(self.context_convs(self.bnc(xc), edge_index, edge_weight_c))\n","        #xo = F.relu(self.objects_convs(self.bno(xo), edge_index, edge_weight_o))\n","        #node_of_graph=xo.shape[0]\n","        #causal_part,causal_node=split_graph(graph_x=xo,node_of_graph=node_of_graph)\n","        #noncausal_part,noncausal_node=split_graph(graph_x=xc,node_of_graph=node_of_graph,type_of_graph=False)\n","\n","        #xc = self.global_pool(xc, batch)\n","        #xo = self.global_pool(xo, batch)\n","\n","        #xc_logis = self.context_readout_layer(xc)\n","        #xo_logis = self.objects_readout_layer(xo)\n","        #xco_logis = self.random_readout_layer(xc, xo, eval_random=eval_random)\n","\n","      # return (xo,edge_index,edge_weight_o),(xc,edge_index,edge_weight_c),batch\n","        return (xo,edge_weight_o),(xc,edge_weight_c),edge_index,batch\n","        #return (causal_attention,causal_part,causal_node),(noncausal_attention,noncausal_part,noncausal_node),batch\n","        #return (xo,causal_part,causal_node),(xc,noncausal_part,noncausal_node),batch\n","\n"]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.datasets import Planetoid\n","from torch_geometric.nn import GCNConv\n","from torch_geometric.data import DataLoader\n","\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n"],"metadata":{"id":"85yBn66PB_QG","executionInfo":{"status":"ok","timestamp":1687075853058,"user_tz":-720,"elapsed":20,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["train_loader = DataLoader(training_final, batch_size=128, shuffle=True)\n","val_loader = DataLoader(valing_final, batch_size=128, shuffle=False)\n","test_loader = DataLoader(testing_final, batch_size=128, shuffle=False)\n","#t_load=[]\n","#for i in train_loader:\n","#  t_load.append(i)\n","#  if(len(t_load)==10000):\n","#    break\n"],"metadata":{"id":"ewtiaFv7H4AC","executionInfo":{"status":"ok","timestamp":1687075853059,"user_tz":-720,"elapsed":21,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"412042c7-2f96-4a22-f34a-a4610cb5575f"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n","  warnings.warn(out)\n"]}]},{"cell_type":"code","source":["number_of_class=10\n","causal_model2 = CausalGAN(7,number_of_class).to(device)\n","predictno_model2=causalpreNO(7,number_of_class).to(device)\n","predictco_model2=causalpreCO(7,number_of_class).to(device)\n","model_optimizer2 = torch.optim.Adam(\n","            list(causal_model2.parameters()) +\n","            list(predictno_model2.parameters())+list(predictco_model2.parameters()),\n","            lr=0.001)\n","Epo=150\n","lr_scheduler = CosineAnnealingLR(model_optimizer2, T_max=Epo, eta_min=1e-6, last_epoch=-1, verbose=False)"],"metadata":{"id":"oqj2r-DqOxaT","executionInfo":{"status":"ok","timestamp":1687075853059,"user_tz":-720,"elapsed":20,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sxpwgMl7_KyG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":15,"metadata":{"id":"NvidGrhFBdK4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687083579961,"user_tz":-720,"elapsed":1922422,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"}},"outputId":"889c5eb2-580e-4751-da25-c0fb20aaa51b"},"outputs":[{"output_type":"stream","name":"stdout","text":["-----training-------0\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [0/150]: 100%|██████████| 79/79 [00:29<00:00,  2.65it/s, loss=0.701]\n"]},{"output_type":"stream","name":"stdout","text":["number of 0 with total loss:0.3561730047192755\n","------valation---------0\n","causal val accuracy:0.2542\n","-----training-------1\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [1/150]: 100%|██████████| 79/79 [00:28<00:00,  2.74it/s, loss=0.0903]\n"]},{"output_type":"stream","name":"stdout","text":["number of 1 with total loss:0.34443934940839116\n","------valation---------1\n","causal val accuracy:0.2628\n","-----training-------2\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [2/150]: 100%|██████████| 79/79 [00:28<00:00,  2.73it/s, loss=2.1]\n"]},{"output_type":"stream","name":"stdout","text":["number of 2 with total loss:0.3575608683160589\n","------valation---------2\n","causal val accuracy:0.2837\n","-----training-------3\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [3/150]: 100%|██████████| 79/79 [00:28<00:00,  2.75it/s, loss=0.687]\n"]},{"output_type":"stream","name":"stdout","text":["number of 3 with total loss:0.3640374260235436\n","------valation---------3\n","causal val accuracy:0.2633\n","-----training-------4\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [4/150]: 100%|██████████| 79/79 [00:29<00:00,  2.70it/s, loss=0.2]\n"]},{"output_type":"stream","name":"stdout","text":["number of 4 with total loss:0.3310958074429367\n","------valation---------4\n","causal val accuracy:0.2741\n","-----training-------5\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [5/150]: 100%|██████████| 79/79 [00:29<00:00,  2.72it/s, loss=0.0893]\n"]},{"output_type":"stream","name":"stdout","text":["number of 5 with total loss:0.3160548550607283\n","------valation---------5\n","causal val accuracy:0.2874\n","-----training-------6\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [6/150]: 100%|██████████| 79/79 [00:28<00:00,  2.73it/s, loss=0.74]\n"]},{"output_type":"stream","name":"stdout","text":["number of 6 with total loss:0.31937033070039145\n","------valation---------6\n","causal val accuracy:0.3117\n","-----training-------7\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [7/150]: 100%|██████████| 79/79 [00:29<00:00,  2.68it/s, loss=1.25]\n"]},{"output_type":"stream","name":"stdout","text":["number of 7 with total loss:0.3370490364636047\n","------valation---------7\n","causal val accuracy:0.2813\n","-----training-------8\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [8/150]: 100%|██████████| 79/79 [00:28<00:00,  2.73it/s, loss=1.69]\n"]},{"output_type":"stream","name":"stdout","text":["number of 8 with total loss:0.33580153275139724\n","------valation---------8\n","causal val accuracy:0.3072\n","-----training-------9\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [9/150]: 100%|██████████| 79/79 [00:28<00:00,  2.74it/s, loss=0.333]\n"]},{"output_type":"stream","name":"stdout","text":["number of 9 with total loss:0.3221995849398118\n","------valation---------9\n","causal val accuracy:0.2854\n","-----training-------10\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [10/150]: 100%|██████████| 79/79 [00:28<00:00,  2.74it/s, loss=0.298]\n"]},{"output_type":"stream","name":"stdout","text":["number of 10 with total loss:0.30229836485431166\n","------valation---------10\n","causal val accuracy:0.304\n","-----training-------11\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [11/150]: 100%|██████████| 79/79 [00:29<00:00,  2.71it/s, loss=0.543]\n"]},{"output_type":"stream","name":"stdout","text":["number of 11 with total loss:0.30432053947750526\n","------valation---------11\n","causal val accuracy:0.2877\n","-----training-------12\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [12/150]: 100%|██████████| 79/79 [00:28<00:00,  2.74it/s, loss=0.312]\n"]},{"output_type":"stream","name":"stdout","text":["number of 12 with total loss:0.29894286903399453\n","------valation---------12\n","causal val accuracy:0.3107\n","-----training-------13\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [13/150]: 100%|██████████| 79/79 [00:29<00:00,  2.72it/s, loss=0.773]\n"]},{"output_type":"stream","name":"stdout","text":["number of 13 with total loss:0.3099064911845364\n","------valation---------13\n","causal val accuracy:0.3259\n","-----training-------14\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [14/150]: 100%|██████████| 79/79 [00:29<00:00,  2.70it/s, loss=0.283]\n"]},{"output_type":"stream","name":"stdout","text":["number of 14 with total loss:0.30482341141640384\n","------valation---------14\n","causal val accuracy:0.3288\n","-----training-------15\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [15/150]: 100%|██████████| 79/79 [00:28<00:00,  2.74it/s, loss=0.618]\n"]},{"output_type":"stream","name":"stdout","text":["number of 15 with total loss:0.29370361340196827\n","------valation---------15\n","causal val accuracy:0.3464\n","-----training-------16\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [16/150]: 100%|██████████| 79/79 [00:28<00:00,  2.76it/s, loss=0.948]\n"]},{"output_type":"stream","name":"stdout","text":["number of 16 with total loss:0.29995754474326025\n","------valation---------16\n","causal val accuracy:0.3395\n","-----training-------17\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [17/150]: 100%|██████████| 79/79 [00:28<00:00,  2.73it/s, loss=0.845]\n"]},{"output_type":"stream","name":"stdout","text":["number of 17 with total loss:0.2854289010732989\n","------valation---------17\n","causal val accuracy:0.3456\n","-----training-------18\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [18/150]: 100%|██████████| 79/79 [00:28<00:00,  2.74it/s, loss=0.442]\n"]},{"output_type":"stream","name":"stdout","text":["number of 18 with total loss:0.28613431185861177\n","------valation---------18\n","causal val accuracy:0.3195\n","-----training-------19\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [19/150]: 100%|██████████| 79/79 [00:28<00:00,  2.73it/s, loss=0.547]\n"]},{"output_type":"stream","name":"stdout","text":["number of 19 with total loss:0.2899460806688176\n","------valation---------19\n","causal val accuracy:0.3455\n","-----training-------20\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [20/150]: 100%|██████████| 79/79 [00:29<00:00,  2.67it/s, loss=0.166]\n"]},{"output_type":"stream","name":"stdout","text":["number of 20 with total loss:0.27448910418190536\n","------valation---------20\n","causal val accuracy:0.315\n","-----training-------21\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [21/150]: 100%|██████████| 79/79 [00:28<00:00,  2.75it/s, loss=0.984]\n"]},{"output_type":"stream","name":"stdout","text":["number of 21 with total loss:0.2807209088078028\n","------valation---------21\n","causal val accuracy:0.3068\n","-----training-------22\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [22/150]: 100%|██████████| 79/79 [00:28<00:00,  2.78it/s, loss=1.31]\n"]},{"output_type":"stream","name":"stdout","text":["number of 22 with total loss:0.2931660977722723\n","------valation---------22\n","causal val accuracy:0.3442\n","-----training-------23\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [23/150]: 100%|██████████| 79/79 [00:28<00:00,  2.72it/s, loss=0.354]\n"]},{"output_type":"stream","name":"stdout","text":["number of 23 with total loss:0.28777273772638057\n","------valation---------23\n","causal val accuracy:0.3448\n","-----training-------24\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [24/150]: 100%|██████████| 79/79 [00:29<00:00,  2.69it/s, loss=0.719]\n"]},{"output_type":"stream","name":"stdout","text":["number of 24 with total loss:0.2768746121208879\n","------valation---------24\n","causal val accuracy:0.3486\n","-----training-------25\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [25/150]: 100%|██████████| 79/79 [00:28<00:00,  2.72it/s, loss=1.03]\n"]},{"output_type":"stream","name":"stdout","text":["number of 25 with total loss:0.28745635400844527\n","------valation---------25\n","causal val accuracy:0.3429\n","-----training-------26\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [26/150]: 100%|██████████| 79/79 [00:28<00:00,  2.73it/s, loss=0.62]\n"]},{"output_type":"stream","name":"stdout","text":["number of 26 with total loss:0.2674151179911215\n","------valation---------26\n","causal val accuracy:0.3538\n","-----training-------27\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [27/150]: 100%|██████████| 79/79 [00:29<00:00,  2.69it/s, loss=0.653]\n"]},{"output_type":"stream","name":"stdout","text":["number of 27 with total loss:0.2638480203061164\n","------valation---------27\n","causal val accuracy:0.352\n","-----training-------28\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [28/150]: 100%|██████████| 79/79 [00:29<00:00,  2.67it/s, loss=0.246]\n"]},{"output_type":"stream","name":"stdout","text":["number of 28 with total loss:0.26050183016665374\n","------valation---------28\n","causal val accuracy:0.3681\n","-----training-------29\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [29/150]: 100%|██████████| 79/79 [00:28<00:00,  2.74it/s, loss=0.142]\n"]},{"output_type":"stream","name":"stdout","text":["number of 29 with total loss:0.2451375041958652\n","------valation---------29\n","causal val accuracy:0.362\n","-----training-------30\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [30/150]: 100%|██████████| 79/79 [00:28<00:00,  2.74it/s, loss=0.732]\n"]},{"output_type":"stream","name":"stdout","text":["number of 30 with total loss:0.2662236877257311\n","------valation---------30\n","causal val accuracy:0.3571\n","-----training-------31\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [31/150]: 100%|██████████| 79/79 [00:28<00:00,  2.73it/s, loss=1.1]\n"]},{"output_type":"stream","name":"stdout","text":["number of 31 with total loss:0.26155814651069764\n","------valation---------31\n","causal val accuracy:0.3533\n","-----training-------32\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [32/150]: 100%|██████████| 79/79 [00:29<00:00,  2.72it/s, loss=0.397]\n"]},{"output_type":"stream","name":"stdout","text":["number of 32 with total loss:0.25099782306182233\n","------valation---------32\n","causal val accuracy:0.3629\n","-----training-------33\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [33/150]: 100%|██████████| 79/79 [00:28<00:00,  2.78it/s, loss=1.17]\n"]},{"output_type":"stream","name":"stdout","text":["number of 33 with total loss:0.26260305130029027\n","------valation---------33\n","causal val accuracy:0.3583\n","-----training-------34\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [34/150]: 100%|██████████| 79/79 [00:28<00:00,  2.74it/s, loss=0.483]\n"]},{"output_type":"stream","name":"stdout","text":["number of 34 with total loss:0.25547734449935866\n","------valation---------34\n","causal val accuracy:0.3842\n","-----training-------35\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [35/150]: 100%|██████████| 79/79 [00:28<00:00,  2.79it/s, loss=0.127]\n"]},{"output_type":"stream","name":"stdout","text":["number of 35 with total loss:0.24557260301294206\n","------valation---------35\n","causal val accuracy:0.3765\n","-----training-------36\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [36/150]: 100%|██████████| 79/79 [00:28<00:00,  2.77it/s, loss=0.591]\n"]},{"output_type":"stream","name":"stdout","text":["number of 36 with total loss:0.24978899663384957\n","------valation---------36\n","causal val accuracy:0.3583\n","-----training-------37\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [37/150]: 100%|██████████| 79/79 [00:28<00:00,  2.77it/s, loss=0.522]\n"]},{"output_type":"stream","name":"stdout","text":["number of 37 with total loss:0.23925798292024225\n","------valation---------37\n","causal val accuracy:0.3625\n","-----training-------38\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [38/150]: 100%|██████████| 79/79 [00:28<00:00,  2.77it/s, loss=0.383]\n"]},{"output_type":"stream","name":"stdout","text":["number of 38 with total loss:0.24103319371425652\n","------valation---------38\n","causal val accuracy:0.3691\n","-----training-------39\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [39/150]: 100%|██████████| 79/79 [00:28<00:00,  2.77it/s, loss=0.602]\n"]},{"output_type":"stream","name":"stdout","text":["number of 39 with total loss:0.24574498681327964\n","------valation---------39\n","causal val accuracy:0.3816\n","-----training-------40\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [40/150]: 100%|██████████| 79/79 [00:29<00:00,  2.72it/s, loss=0.166]\n"]},{"output_type":"stream","name":"stdout","text":["number of 40 with total loss:0.2363875018833559\n","------valation---------40\n","causal val accuracy:0.3776\n","-----training-------41\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [41/150]: 100%|██████████| 79/79 [00:28<00:00,  2.73it/s, loss=0.492]\n"]},{"output_type":"stream","name":"stdout","text":["number of 41 with total loss:0.23886202341770824\n","------valation---------41\n","causal val accuracy:0.3671\n","-----training-------42\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [42/150]: 100%|██████████| 79/79 [00:28<00:00,  2.74it/s, loss=0.424]\n"]},{"output_type":"stream","name":"stdout","text":["number of 42 with total loss:0.2388356049792676\n","------valation---------42\n","causal val accuracy:0.3518\n","-----training-------43\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [43/150]: 100%|██████████| 79/79 [00:28<00:00,  2.74it/s, loss=0.18]\n"]},{"output_type":"stream","name":"stdout","text":["number of 43 with total loss:0.23593236958678765\n","------valation---------43\n","causal val accuracy:0.3388\n","-----training-------44\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [44/150]: 100%|██████████| 79/79 [00:28<00:00,  2.73it/s, loss=0.193]\n"]},{"output_type":"stream","name":"stdout","text":["number of 44 with total loss:0.2266352858535851\n","------valation---------44\n","causal val accuracy:0.3725\n","-----training-------45\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [45/150]: 100%|██████████| 79/79 [00:28<00:00,  2.78it/s, loss=0.103]\n"]},{"output_type":"stream","name":"stdout","text":["number of 45 with total loss:0.22486794136370283\n","------valation---------45\n","causal val accuracy:0.3768\n","-----training-------46\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [46/150]: 100%|██████████| 79/79 [00:28<00:00,  2.72it/s, loss=0.0907]\n"]},{"output_type":"stream","name":"stdout","text":["number of 46 with total loss:0.22244984024687658\n","------valation---------46\n","causal val accuracy:0.3831\n","-----training-------47\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [47/150]: 100%|██████████| 79/79 [00:29<00:00,  2.71it/s, loss=0.816]\n"]},{"output_type":"stream","name":"stdout","text":["number of 47 with total loss:0.22965545103519777\n","------valation---------47\n","causal val accuracy:0.3634\n","-----training-------48\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [48/150]: 100%|██████████| 79/79 [00:28<00:00,  2.75it/s, loss=0.736]\n"]},{"output_type":"stream","name":"stdout","text":["number of 48 with total loss:0.23309577794014652\n","------valation---------48\n","causal val accuracy:0.3702\n","-----training-------49\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [49/150]: 100%|██████████| 79/79 [00:28<00:00,  2.74it/s, loss=0.317]\n"]},{"output_type":"stream","name":"stdout","text":["number of 49 with total loss:0.2185090932099125\n","------valation---------49\n","causal val accuracy:0.3697\n","-----training-------50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [50/150]: 100%|██████████| 79/79 [00:29<00:00,  2.72it/s, loss=0.415]\n"]},{"output_type":"stream","name":"stdout","text":["number of 50 with total loss:0.2322648847593537\n","------valation---------50\n","causal val accuracy:0.3719\n","-----training-------51\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [51/150]: 100%|██████████| 79/79 [00:28<00:00,  2.75it/s, loss=0.379]\n"]},{"output_type":"stream","name":"stdout","text":["number of 51 with total loss:0.21857946818765206\n","------valation---------51\n","causal val accuracy:0.3942\n"]}],"source":["import time\n","import json\n","\n","loss_value=[]\n","loss_value_valation=[]\n","c=0.5\n","o=1\n","co=0.5\n","def num_graphs(data):\n","  if data.batch is not None:\n","      return data.num_graphs\n","  else:\n","      return data.x.size(0)\n","from tqdm import tqdm\n","\n","for epoch in range(Epo):\n","  #model.train()\n","  causal_model2.train()\n","  predictno_model2.train()\n","  predictco_model2.train()\n","  total_loss = 0\n","  total_loss_c = 0\n","  total_loss_o = 0\n","  total_loss_co = 0\n","  correct_o = 0\n","  nb=0\n","  print(f\"-----training-------{epoch}\")\n","  loop = tqdm(enumerate(train_loader),total=len(train_loader))\n","  for it, data in loop:\n","#  for it, data in enumerate(train_loader):\n","      nb+=1\n","      model_optimizer2.zero_grad()\n","      data = data.to(device)\n","\n","      one_hot_target = data.y.view(-1)\n","      #causal,noncausal,batch=causal_model(data)\n","      (xo,edge_weight_o),(xc,edge_weight_c),edge_index,batch=causal_model2(data)\n","      #c_logs, o_logs, co_logs = predict_model(causal=causal,noncausal=noncausal,batch=batch,causal_edge_weight=edge_weight_causal,noncausal_edge_weight=edge_weight_noncausal,edge_index=edge_index)\n","      c_logs,co_logs = predictno_model2(causal=xo,noncausal=xc,edge_index=edge_index,causal_edge_weight=edge_weight_o,noncausal_edge_weight=edge_weight_c,batch=batch)\n","      o_logs=predictco_model2(causal=xo,edge_index=edge_index,causal_edge_weight=edge_weight_o,batch=batch)\n","      uniform_target = torch.ones_like(c_logs, dtype=torch.float).to(device)/number_of_class\n","     # sim=F.cosine_similarity(causal_attention,noncausal_attention).mean()\n","      #print(o_logs)\n","      #print(type(o_logs))\n","      c_loss = F.kl_div(c_logs, uniform_target,reduction='batchmean')\n","      #print(f'causal loss{c_loss}')\n","      o_loss = F.nll_loss(o_logs,one_hot_target)\n","      #print(f'noncausal loss{o_loss}')\n","      co_loss = F.nll_loss(co_logs,one_hot_target)\n","      #print(f'com causal loss{co_loss}')\n","      loss = c * c_loss + o * o_loss + co * co_loss\n","\n","      pred_o = o_logs.max(1)[1]\n","      correct_o += pred_o.eq(data.y.view(-1)).sum().item()\n","      loss.backward()\n","      total_loss += loss.item() #* num_graphs(data)\n","      total_loss_c += c_loss.item() #* num_graphs(data)\n","      total_loss_o += o_loss.item() #* num_graphs(data)\n","      total_loss_co += co_loss.item()# * num_graphs(data)\n","      model_optimizer2.step()\n","      loop.set_description(f\"Epoch [{epoch}/{Epo}]\")\n","      loop.set_postfix(loss = loss.item())\n","\n","  #num = len(train_loader.dataset)\n","  lr_scheduler.step()\n","  total_loss = total_loss / nb\n","  total_loss_c = total_loss_c / nb\n","  print(f'number of {epoch} with total loss:{total_loss}')\n","  loss_value.append(total_loss)\n","  total_loss_o = total_loss_o / nb\n","  total_loss_co = total_loss_co / nb\n","  correct_o = correct_o / nb\n","  with torch.no_grad():\n","    correct=0\n","    correct_c=0\n","    correct_o=0\n","    print(f\"------valation---------{epoch}\")\n","    causal_model2.eval()\n","    predictno_model2.eval()\n","    predictco_model2.eval()\n","    for data in test_loader:\n","      (xo,edge_weight_o),(xc,edge_weight_c),edge_index,batch=causal_model2(data)\n","      #c_logs, o_logs, co_logs = predict_model(causal=causal,noncausal=noncausal,batch=batch,causal_edge_weight=edge_weight_causal,noncausal_edge_weight=edge_weight_noncausal,edge_index=edge_index)\n","      c_logs,co_logs = predictno_model2(causal=xo,noncausal=xc,edge_index=edge_index,causal_edge_weight=edge_weight_o,noncausal_edge_weight=edge_weight_c,batch=batch)\n","      o_logs=predictco_model2(causal=xo,edge_index=edge_index,causal_edge_weight=edge_weight_o,batch=batch)\n","      pred = co_logs.max(1)[1]\n","      pred_c = c_logs.max(1)[1]\n","      pred_o = o_logs.max(1)[1]\n","      correct += pred.eq(data.y.view(-1)).sum().item()\n","      correct_c += pred_c.eq(data.y.view(-1)).sum().item()\n","      correct_o += pred_o.eq(data.y.view(-1)).sum().item()\n","    acc_co = correct / len(test_loader.dataset)\n","    acc_c = correct_c / len(test_loader.dataset)\n","    acc_o = correct_o / len(test_loader.dataset)\n","    print(f\"causal val accuracy:{acc_co}\")\n","    loss_value_valation.append(acc_co)\n","    dictionary={\"number of epoch\":epoch,\n","                \"training loss list\":loss_value,\n","                \"valation accuracy list\":loss_value_valation}\n","\n","    # Serializing json\n","    json_object = json.dumps(dictionary,indent=3)\n","\n","    # Writing to sample.json\n","   # with open(\"/content/drive/MyDrive/running_cal_mnist/number_tl_va_e08.json\", \"w\") as outfile:\n","   #     outfile.write(json_object)\n","\n","    torch.save({\n","            'causal_model.state_dic': causal_model2.state_dict(),\n","            'predictco_model_state_dict()': predictco_model2.state_dict(),\n","            'predictno_model_state_dict()': predictno_model2.state_dict(),\n","            'opt':model_optimizer2.state_dict()\n","            }, '/content/drive/MyDrive/running_cal_mnist/allmodel_cal_0933.pt')\n","    if(epoch>50):\n","      check=abs(acc_o-loss_value_valation[len(loss_value_valation)-20])/20\n","      if(check<=0.01):\n","        break\n","\n","\n"]},{"cell_type":"code","source":["aaa"],"metadata":{"id":"_oB1VVj59n_W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoint = torch.load('/content/drive/MyDrive/running_cal_mnist/allmodel_cal_09.pt')\n","causal_model2.load_state_dict(checkpoint['causal_model.state_dic'])\n","predictco_model2.load_state_dict(checkpoint['predictco_model_state_dict()'])\n","predictno_model2.load_state_dict(checkpoint['predictno_model_state_dict()'])"],"metadata":{"id":"A1xSjQ6XrNM9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_loader = DataLoader(testing_final, batch_size=1, shuffle=False)\n","\n","correct=0\n","correct_c=0\n","correct_o=0\n","print(f\"------test---------{0000000}\")\n","causal_model2.eval()\n","predictno_model2.eval()\n","predictco_model2.eval()\n","for data in test_loader:\n","  (xo,edge_weight_o),(xc,edge_weight_c),edge_index,batch=causal_model2(data)\n","  #c_logs, o_logs, co_logs = predict_model(causal=causal,noncausal=noncausal,batch=batch,causal_edge_weight=edge_weight_causal,noncausal_edge_weight=edge_weight_noncausal,edge_index=edge_index)\n","  c_logs,co_logs = predictno_model2(causal=xo,noncausal=xc,edge_index=edge_index,causal_edge_weight=edge_weight_o,noncausal_edge_weight=edge_weight_c,batch=batch)\n","  o_logs=predictco_model2(causal=xo,edge_index=edge_index,causal_edge_weight=edge_weight_o,batch=batch)\n","  pred = co_logs.max(1)[1]\n","  pred_c = c_logs.max(1)[1]\n","  pred_o = o_logs.max(1)[1]\n","  correct += pred.eq(data.y.view(-1)).sum().item()\n","  correct_c += pred_c.eq(data.y.view(-1)).sum().item()\n","  correct_o += pred_o.eq(data.y.view(-1)).sum().item()\n","acc_co = correct / len(test_loader.dataset)\n","acc_c = correct_c / len(test_loader.dataset)\n","acc_o = correct_o / len(test_loader.dataset)\n","print(f\"combian causal test accuracy:{acc_co}\")\n","print(f\"causal test accuracy:{acc_o}\")\n","#dictionary={\"number of epoch\":epoch,\n","#            \"training loss list\":loss_value,\n","#            \"valation accuracy list\":loss_value_valation,\n","#            \"test accuracy value\":acc_co}\n","\n","# Serializing json\n","#json_object = json.dumps(dictionary,indent=4)\n","\n","# Writing to sample.json\n","#with open(\"/content/drive/MyDrive/running_cal_mnist/number_tl_va_e09.json\", \"w\") as outfile:\n","#    outfile.write(json_object)\n"],"metadata":{"id":"tSV_5LT74ACj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_loader = DataLoader(testing_final[:10], batch_size=1, shuffle=False)"],"metadata":{"id":"jlvJvXaZ_M77"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aREXcYIU326g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch_geometric.utils import to_dgl,from_dgl,k_hop_subgraph,is_undirected"],"metadata":{"id":"KKOxJy_07zx5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tey=tol[0]\n","causal_model2.eval()\n","predictno_model2.eval()\n","predictco_model2.eval()\n","(xo,edge_weight_o),(xc,edge_weight_c),edge_index,batch=causal_model2(tey)\n","lab=tey.y\n","tey"],"metadata":{"id":"OPUxcF3U2BwK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xo"],"metadata":{"id":"yHJRF3XfMZJw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import math\n","import math\n","import numpy as np\n","from torch_geometric.utils import (remove_self_loops, degree,\n","                                   batched_negative_sampling)\n","from torch_geometric.utils.num_nodes import maybe_num_nodes\n","def split_batch(g):\n","    split = degree(g.batch[g.edge_index[0]], dtype=torch.long).tolist()\n","    edge_indices = torch.split(g.edge_index, split, dim=1)\n","    num_nodes = degree(g.batch, dtype=torch.long)\n","    cum_nodes = torch.cat([g.batch.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]])\n","    num_edges = torch.tensor([e.size(1) for e in edge_indices], dtype=torch.long).to(g.x.device)\n","    cum_edges = torch.cat([g.batch.new_zeros(1), num_edges.cumsum(dim=0)[:-1]])\n","\n","    return edge_indices, num_nodes, cum_nodes, num_edges, cum_edges\n","\n","def split_graph(data, edge_score, ratio):\n","    causal_edge_index = torch.LongTensor([[],[]]).to(data.x.device)\n","    causal_edge_weight = torch.tensor([]).to(data.x.device)\n","    causal_edge_attr = torch.tensor([]).to(data.x.device)\n","    conf_edge_index = torch.LongTensor([[],[]]).to(data.x.device)\n","    conf_edge_weight = torch.tensor([]).to(data.x.device)\n","    conf_edge_attr = torch.tensor([]).to(data.x.device)\n","\n","    edge_indices, _, _, num_edges, cum_edges = split_batch(data)\n","    for edge_index, N, C in zip(edge_indices, num_edges, cum_edges):\n","        n_reserve =  int(ratio * N)\n","        edge_attr = data.edge_attr[C:C+N]\n","        single_mask = edge_score[C:C+N]\n","        single_mask_detach = edge_score[C:C+N].detach().cpu().numpy()\n","        rank = np.argpartition(-single_mask_detach, n_reserve)\n","        idx_reserve, idx_drop = rank[:n_reserve], rank[n_reserve:]\n","\n","        causal_edge_index = torch.cat([causal_edge_index, edge_index[:, idx_reserve]], dim=1)\n","        conf_edge_index = torch.cat([conf_edge_index, edge_index[:, idx_drop]], dim=1)\n","\n","        causal_edge_weight = torch.cat([causal_edge_weight, single_mask[idx_reserve]])\n","        conf_edge_weight = torch.cat([conf_edge_weight,  -1 * single_mask[idx_drop]])\n","\n","        causal_edge_attr = torch.cat([causal_edge_attr, edge_attr[idx_reserve]])\n","        conf_edge_attr = torch.cat([conf_edge_attr, edge_attr[idx_drop]])\n","    return (causal_edge_index, causal_edge_attr, causal_edge_weight), \\\n","        (conf_edge_index, conf_edge_attr, conf_edge_weight)\n","\n","\n"],"metadata":{"id":"cF-3qAELU2BN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ratio=0.1\n","\n","causal_edge_index = torch.LongTensor([[],[]])\n","causal_edge_weight = torch.tensor([])\n","causal_edge_attr = torch.tensor([])\n","\n","edge_indices, _, _, num_edges, cum_edges = split_batch(tey)\n","split_batch(tey)"],"metadata":{"id":"PykJPJ9zg7qw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.topk(xc[non_sub_nodes].mean(axis=1),int(len(non_sub_nodes)*0.1))"],"metadata":{"id":"VILmbOfFGhuH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ratio=0.5\n","\n","causal_edge_index = torch.LongTensor([[],[]])\n","causal_edge_weight = torch.tensor([])\n","conf_edge_index = torch.LongTensor([[],[]])\n","conf_edge_weight = torch.tensor([])\n","\n","#causal_edge_attr = torch.tensor([])\n","topnode=torch.LongTensor([])\n","notopnode=torch.LongTensor([])\n","\n","edge_indices, _, _, num_edges, cum_edges = split_batch(tey)\n","for edge_index, N, C in zip(edge_indices, num_edges, cum_edges):\n","    n_reserve =  int(ratio * N)\n","    edge_attr = edge_weight_o[C:C+N]\n","    single_mask = edge_weight_o[C:C+N]\n","    single_mask_detach = edge_weight_o[C:C+N].detach().cpu().numpy()\n","    rank = np.argpartition(-single_mask_detach, n_reserve)\n","    idx_reserve, idx_drop = rank[:n_reserve], rank[n_reserve:]\n","\n","    causal_edge_index = torch.cat([causal_edge_index, edge_index[:, idx_reserve]], dim=1)\n","    conf_edge_index = torch.cat([conf_edge_index, edge_index[:, idx_drop]], dim=1)\n","    #print(causal_edge_index)\n","    causal_edge_weight = torch.cat([causal_edge_weight, single_mask[idx_reserve]])\n","    conf_edge_weight = torch.cat([conf_edge_weight,  -1 * single_mask[idx_drop]])\n","    causal_sub_nodes = torch.unique(edge_index[:, idx_reserve])\n","    non_sub_nodes=torch.unique(edge_index[:, idx_drop])\n","\n","    print(f'non_subnode:{non_sub_nodes}')\n","   # print(f'non weight:{ -1 * single_mask[idx_drop]}')\n","    print(f'subnode:{causal_sub_nodes}')\n","    #print(f'c subnode weight:{single_mask[idx_reserve]}')\n","    topnon=torch.topk(xc[non_sub_nodes].mean(axis=1),int(len(non_sub_nodes)*0.1))[1]\n","    top=torch.topk(xo[causal_sub_nodes].mean(axis=1),int(len(causal_sub_nodes)*0.1))[1]\n","    topnode = torch.cat([topnode, causal_sub_nodes[top]])\n","    notopnode=torch.cat([notopnode, non_sub_nodes[topnon]])\n","    #print(topnode)\n","    print(f'top node:{topnode}')\n","    print(f'nontop node:{notopnode}')\n","\n","\n","   #causal_edge_attr = torch.cat([causal_edge_attr, edge_attr[idx_reserve]])\n","    #conf_edge_attr = torch.cat([conf_edge_attr, edge_attr[idx_drop]])"],"metadata":{"id":"bAatC9ITU62d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def split_graph_bybatch(edge_weight_o,xo,xc,data,ratio=0.5):\n","  causal_edge_index = torch.LongTensor([[],[]])\n","  causal_edge_weight = torch.tensor([])\n","  conf_edge_index = torch.LongTensor([[],[]])\n","  conf_edge_weight = torch.tensor([])\n","\n","  #causal_edge_attr = torch.tensor([])\n","  topnode=torch.LongTensor([])\n","  notopnode=torch.LongTensor([])\n","\n","  edge_indices, _, _, num_edges, cum_edges = split_batch(data)\n","  for edge_index, N, C in zip(edge_indices, num_edges, cum_edges):\n","      n_reserve =  int(ratio * N)\n","      edge_attr = edge_weight_o[C:C+N]\n","      single_mask = edge_weight_o[C:C+N]\n","      single_mask_detach = edge_weight_o[C:C+N].detach().cpu().numpy()\n","      rank = np.argpartition(-single_mask_detach, n_reserve)\n","      idx_reserve, idx_drop = rank[:n_reserve], rank[n_reserve:]\n","\n","      causal_edge_index = torch.cat([causal_edge_index, edge_index[:, idx_reserve]], dim=1)\n","      conf_edge_index = torch.cat([conf_edge_index, edge_index[:, idx_drop]], dim=1)\n","      #print(causal_edge_index)\n","      causal_edge_weight = torch.cat([causal_edge_weight, single_mask[idx_reserve]])\n","      conf_edge_weight = torch.cat([conf_edge_weight,  -1 * single_mask[idx_drop]])\n","      causal_sub_nodes = torch.unique(edge_index[:, idx_reserve])\n","      non_sub_nodes=torch.unique(edge_index[:, idx_drop])\n","\n","      #print(f'non_subnode:{non_sub_nodes}')\n","    # print(f'non weight:{ -1 * single_mask[idx_drop]}')\n","      #print(f'subnode:{causal_sub_nodes}')\n","      #print(f'c subnode weight:{single_mask[idx_reserve]}')\n","      topnon=torch.topk(xc[non_sub_nodes].mean(axis=1),int(len(causal_sub_nodes)*0.08))[1]\n","      top=torch.topk(xo[causal_sub_nodes].mean(axis=1),int(len(causal_sub_nodes)*0.08))[1]\n","      topnode = torch.cat([topnode, causal_sub_nodes[top]])\n","      notopnode=torch.cat([notopnode, non_sub_nodes[topnon]])\n","      #print(topnode)\n","      #print(f'top node:{topnode}')\n","      #print(f'nontop node:{notopnode}')\n","  return (causal_edge_index,causal_edge_weight,topnode),(conf_edge_index,conf_edge_weight,notopnode)\n"],"metadata":{"id":"vj3tKKbcIkvw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["(causal_edge_index,causal_edge_weight,topnode),(conf_edge_index,conf_edge_weight,notopnode)=split_graph_bybatch(edge_weight_o,xo,xc,tey,ratio=0.5)"],"metadata":{"id":"FzlVQzDbWk9F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["is_undirected(causal_edge_index,causal_edge_weight)"],"metadata":{"id":"Ed6ZtpAZm_wT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tey.batch[topnode]\n","tey.y"],"metadata":{"id":"w2BB09CSb89L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","batchnumber_index"],"metadata":{"id":"VCB59kXpd_2q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["select_node_batch_number"],"metadata":{"id":"Ej75nPYZ6qnA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["number_of_g_inbatch=2\n","select_node_batch_number=tey.batch[topnode]\n","tt = []\n","for i in range(number_of_g_inbatch):\n","  tt.append((select_node_batch_number == i).unsqueeze(0))\n","batchnumber_index=torch.cat(tt,dim=0).sum(dim=1)\n","number_of_sample=int(min(batchnumber_index))\n","n=0\n","while(n<number_of_sample):\n","  n+=1\n","  for i in select_node_batch_number:\n","\n","\n"],"metadata":{"id":"RZ6UtvjEcsJK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["topnode"],"metadata":{"id":"dG3CWgSqbEH5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["notopnode"],"metadata":{"id":"LluLpAqybFOj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def k_hop_subgraph(node_idx,num_hops,edge_index,ew,relabel_nodes=False,num_nodes = None):\n","\n","    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n","\n","\n","    col, row = edge_index\n","\n","    node_mask = row.new_empty(num_nodes, dtype=torch.bool)\n","    edge_mask = row.new_empty(row.size(0), dtype=torch.bool)\n","\n","    if isinstance(node_idx, (int, list, tuple)):\n","        node_idx = torch.tensor([node_idx], device=row.device).flatten()\n","    else:\n","        node_idx = node_idx.to(row.device)\n","\n","    subsets = [node_idx]\n","\n","    for _ in range(num_hops):\n","        node_mask.fill_(False)\n","        node_mask[subsets[-1]] = True\n","        torch.index_select(node_mask, 0, row, out=edge_mask)\n","        subsets.append(col[edge_mask])\n","\n","    subset, inv = torch.cat(subsets).unique(return_inverse=True)\n","    inv = inv[:node_idx.numel()]\n","\n","    node_mask.fill_(False)\n","    node_mask[subset] = True\n","\n","\n","    edge_mask = node_mask[row] & node_mask[col]\n","\n","    edge_index = edge_index[:, edge_mask]\n","\n","    edge_weight=torch.masked_select(ew,edge_mask)\n","    if relabel_nodes:\n","        node_idx = row.new_full((num_nodes, ), -1)\n","        node_idx[subset] = torch.arange(subset.size(0), device=row.device)\n","        edge_index = node_idx[edge_index]\n","\n","    return subset, edge_index, inv,edge_weight\n"],"metadata":{"id":"OadGHYtaWuQw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subset, edge_index,_,edge_weight=k_hop_subgraph(8,5,causal_edge_index,causal_edge_weight,relabel_nodes=False)\n","edge_weight"],"metadata":{"id":"XuG_Pk58jNz-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch_geometric.data import Data,Batch"],"metadata":{"id":"-OlnhS9K5vjr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Data(x=xo[subset],edge_index=edge_index,edge_attr=edge_weight,y=0)"],"metadata":{"id":"Gmjj2Nj-6F6D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def combine_subg(databatch,select_node,edge_indexn,eweight,tragety):\n","  undata=[]\n","  select_node_batch_number=databatch[select_node]\n","  for i in range(len(select_node)):\n","    print(i)\n","    #subset,edge_index,_,edge_weight=k_hop_subgraph(int(select_node[i]),5,edge_index,eweight,relabel_nodes=False)\n","    subset, edge_index,_,edge_weight=k_hop_subgraph(int(select_node[i]),5,edge_indexn,eweight,relabel_nodes=False)\n","    undata.append(Data(x=xo[subset],edge_index=edge_index,edge_attr=edge_weight,y=tragety[int(select_node_batch_number[i])]))\n","  #d=DataLoader(undata,batch_size=int(len(select_node)/4),shuffle=True)\n","  fin=Batch.from_data_list(undata)\n","  return fin\n","\n","\n","combine_subg(tey.batch,topnode,causal_edge_index,causal_edge_weight,tey.y)"],"metadata":{"id":"n-pC6KqO4d2p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["d=DataLoader(undata,batch_size=len(topnode))\n","Batch.from_data_list(undata)\n"],"metadata":{"id":"4-pX449A6ykE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in d:\n","  print(i)"],"metadata":{"id":"qvbVciiz8G84"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#xo[subset]\n","torch.masked_select(causal_edge_weight,edge_mask)"],"metadata":{"id":"RiwrJa2i1WRM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["k_hop_subgraph(16,5,causal_edge_index)[0]"],"metadata":{"id":"FkvPTCJXu4RS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["topnode"],"metadata":{"id":"VoLyOcZsoPP3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch[topnode]"],"metadata":{"id":"mIhvBgS3h6B-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NPdl_B7mLjUn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subn, _, _, broken_mask=bid_k_hop_subgraph(16,3, causal_edge_index, relabel_nodes=False,num_nodes=None)"],"metadata":{"id":"lphTX9-CPnBE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xo[subn]"],"metadata":{"id":"NOqPiBikOsaZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.masked_select(causal_edge_weight,broken_mask)"],"metadata":{"id":"scVRjfGnt38V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["edge_index"],"metadata":{"id":"y4G5IUwItxm1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["causal_edge_index"],"metadata":{"id":"JZ0kUGERtiaG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["causal_edge_weight"],"metadata":{"id":"pU_ciLHLtpt4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["causal_edge_weight[edge_mask]"],"metadata":{"id":"EDo7JdULs-OI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["graph_nodes_number=xo.shape[0]\n","graph_nodes_number"],"metadata":{"id":"ooJwMRHONDvG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7AILi6ZqTW_9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xo.sum(axis=1)[48:]"],"metadata":{"id":"LPxHhbk8S9qZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["top10=torch.topk(xo.mean(axis=1),int(graph_nodes_number*0.2))"],"metadata":{"id":"THYixZumSbfJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["top10"],"metadata":{"id":"54ZzwIdVSf7e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#lab\n","\n","G = dgl.DGLGraph()\n","G.add_nodes(graph_nodes_number)\n","G.ndata['x']=xo\n","G.add_edges(edge_index[0],edge_index[1])\n","G.edata['edge_attr']=edge_weight_o\n","G.ndata['batch']=batch\n","G"],"metadata":{"id":"9TPNsxYY_4Mq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["G.ndata['x']"],"metadata":{"id":"Wm3trUbcZbU6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["offset = 0\n","nodes_offset = 0\n","new_data_list = []\n","#label_list = []\n","for data, label in zip(data_list, batch_labels):\n","    num_edges = data.number_of_edges()\n","    edge_score = data_mask[offset:offset + num_edges]\n","    mean_score = torch.mean(edge_score)\n","    sorted_score, index = torch.sort(edge_score.view(-1))\n","\n","    prune_num_edges = int(torch.sum(torch.lt(edge_score, mean_score)))\n","    #_, index = torch.sort(edge_score.view(-1))\n","    prune_index = index[:prune_num_edges]\n","    data.remove_edges(prune_index)\n","    #data.edata['mask'] = edge_score.cpu()\n","    isolated_nodes = ((data.in_degrees() == 0) & (data.out_degrees() == 0)).nonzero().squeeze(1)\n","    data.remove_nodes(isolated_nodes)\n","    new_data_list.append(data)\n","    offset += num_edges"],"metadata":{"id":"fuzunOpTZrXV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["edge_score = edge_weight_o.clone()\n","mean_score = torch.mean(edge_score)\n","sorted_score, index = torch.sort(edge_score.view(-1))\n","prune_num_edges = int(torch.sum(torch.lt(edge_score, mean_score)))\n","prune_index=index[:prune_num_edges]\n","G.remove_edges(prune_index)\n"],"metadata":{"id":"XHf_YCe-OuWP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#tey=to_dgl(tey)"],"metadata":{"id":"2yx5NSWz8Qd-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tey"],"metadata":{"id":"jz8tkfqa8Vrl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tey.edges()"],"metadata":{"id":"rZmv5dvE4VgZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sg.edges()"],"metadata":{"id":"3RRNP3S45HKv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sg, inverse_indices = dgl.khop_out_subgraph(tey, 0, k=1)"],"metadata":{"id":"eh35WcgA45z9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["edge_weight_o.view(-1)"],"metadata":{"id":"VKJXmfeL4-i1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def subg_edge(data,weight):\n","  re_data=data.clone()\n","  edge_score = weight.clone()\n","  re_data=to_dgl(re_data)\n","  mean_score = torch.mean(edge_score)\n","  sorted_score, index = torch.sort(edge_score.view(-1))\n","  prune_num_edges = int(torch.sum(torch.lt(edge_score, mean_score)))\n","  prune_index=index[:prune_num_edges]\n","  re_data.remove_edges(prune_index)\n"," # isolated_nodes = ((re_data.in_degrees() == 0) & (re_data.out_degrees() == 0)).nonzero().squeeze(1)\n"," # re_data.remove_nodes(isolated_nodes)\n","  re_data=from_dgl(re_data)\n","  return re_data\n"],"metadata":{"id":"-wXdOwvpGzmZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def subg_node(data,top_select_number):\n","  node_number=data.x.shape[0]\n","  re_data=data.clone()\n","  node_weight = data.x.mean(axis=1)\n","  re_data=to_dgl(re_data)\n","  #mean_score = torch.mean(edge_score)\n","  prune_number_node=node_number-top_select_number\n","  sorted_score, index = torch.sort(node_weight.view(-1))\n","  #prune_num_edges = int(torch.sum(torch.lt(edge_score, mean_score)))\n","  prune_index=index[:prune_number_node]\n","  re_data.remove_nodes(prune_index)\n","  re_data=from_dgl(re_data)\n","  return re_data\n"],"metadata":{"id":"u4GXsv7hTY3o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subg_node(tey,10).edge_index"],"metadata":{"id":"f4qk1oePWGbB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sorted_score, index=torch.topk(edge_score.view(-1),10)\n","index"],"metadata":{"id":"4UzzS5FXNwyJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_edges = tey.number_of_edges()\n","edge_score = edge_weight_o\n","mean_score = torch.mean(edge_score)\n","sorted_score, index = torch.sort(edge_score.view(-1))\n","\n","prune_num_edges = int(torch.sum(torch.lt(edge_score, mean_score)))\n","prune_index=index[:prune_num_edges]\n"],"metadata":{"id":"LunJQiYE5XKp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tey.remove_edges(prune_index)"],"metadata":{"id":"zdL1s6zc86ut"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["isolated_nodes = ((tey.in_degrees() == 0) & (tey.out_degrees() == 0)).nonzero().squeeze(1)\n"],"metadata":{"id":"pJHy7y4S9CtX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["isolated_nodes"],"metadata":{"id":"hiVuUgjkUPkI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tey.remove_nodes(isolated_nodes)\n","tey=from_dgl(tey)\n","tey"],"metadata":{"id":"QPPrm-Ve9O8j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tey.batch"],"metadata":{"id":"vA2GADW_94Yo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["(xo,edge_weight_o),(xc,edge_weight_c),edge_index,batch=causal_model2(tey)\n","\n","#c_logs, o_logs, co_logs = predict_model(causal=causal,noncausal=noncausal,batch=batch,causal_edge_weight=edge_weight_causal,noncausal_edge_weight=edge_weight_noncausal,edge_index=edge_index)\n","c_logs,co_logs = predictno_model2(causal=xo,noncausal=xc,edge_index=edge_index,causal_edge_weight=edge_weight_o,noncausal_edge_weight=edge_weight_c,batch=batch)\n","o_logs=predictco_model2(causal=xo,edge_index=edge_index,causal_edge_weight=edge_weight_o,batch=batch)\n","pred = co_logs.max(1)[1]\n","pred_o = o_logs.max(1)[1]\n","print(pred.eq(lab.view(-1)).sum().item())\n","print(pred_o)"],"metadata":{"id":"5j7nvwA79pQt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred.eq(lab.view(-1))"],"metadata":{"id":"DOl4jg72DnQ2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["offset = 0\n","nodes_offset = 0\n","new_data_list = []\n","label_list = []\n","for data, label in zip(data_list, batch_labels):\n","    num_edges = data.number_of_edges()\n","    edge_score = edge_weight_o\n","    mean_score = torch.mean(edge_score)\n","    sorted_score, index = torch.sort(edge_score.view(-1))\n","\n","    prune_num_edges = int(torch.sum(torch.lt(edge_score, mean_score)))\n","    #_, index = torch.sort(edge_score.view(-1))\n","    prune_index = index[:prune_num_edges]\n","    data.remove_edges(prune_index)\n","    #data.edata['mask'] = edge_score.cpu()\n","    isolated_nodes = ((data.in_degrees() == 0) & (data.out_degrees() == 0)).nonzero().squeeze(1)\n","    data.remove_nodes(isolated_nodes)\n","    new_data_list.append(data)\n","    label_list.append(label)\n","    offset += num_edges"],"metadata":{"id":"R5KFVnly4qTf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pruning_batch_data_from_mask_cp(data_list, batch_labels, data_mask):\n","    offset = 0\n","    nodes_offset = 0\n","    new_data_list = []\n","    label_list = []\n","    for data, label in zip(data_list, batch_labels):\n","        num_edges = data.number_of_edges()\n","        edge_score = data_mask[offset:offset + num_edges]\n","        mean_score = torch.mean(edge_score)\n","        sorted_score, index = torch.sort(edge_score.view(-1))\n","\n","        prune_num_edges = int(torch.sum(torch.lt(edge_score, mean_score)))\n","        #_, index = torch.sort(edge_score.view(-1))\n","        prune_index = index[:prune_num_edges]\n","        data.remove_edges(prune_index)\n","        #data.edata['mask'] = edge_score.cpu()\n","        isolated_nodes = ((data.in_degrees() == 0) & (data.out_degrees() == 0)).nonzero().squeeze(1)\n","        data.remove_nodes(isolated_nodes)\n","        new_data_list.append(data)\n","        label_list.append(label)\n","        offset += num_edges\n","\n","    return new_data_list, label_list"],"metadata":{"id":"gnmvkjiS4Y4L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["(xo,edge_weight_o),(xc,edge_weight_c),edge_index,batch=causal_model2(tey)"],"metadata":{"id":"9rVpyGS72GVY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["edge_weight_o.shape"],"metadata":{"id":"R0_MZabE2ZXg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.topk(edge_weight_o,10)"],"metadata":{"id":"KNPE8Uw62hm3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tey.edge_index"],"metadata":{"id":"xlfHsqUh3Oz4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# Opening JSON file\n","f = open('/content/drive/MyDrive/running_cal_mnist/number_tl_va_e09.json')\n","\n","# returns JSON object as\n","# a dictionary\n","data = json.load(f)"],"metadata":{"id":"lSLiQ0cmqKvv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_valuefi=data['training loss list']\n","for i in range(len(loss_valuefi)):\n","  loss_valuefi[i]=float(loss_valuefi[i])\n","plt.plot(np.array(loss_valuefi))\n","plt.show()"],"metadata":{"id":"vrirFdpoqqdO"},"execution_count":null,"outputs":[]}]}