{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50838,"status":"ok","timestamp":1687083875211,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"},"user_tz":-720},"id":"xVEXjGk1WGRp","outputId":"3ad8d217-945c-4efe-92d1-aaa34616c960"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.0.1+cu118\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\n","Collecting pyg_lib\n","  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/pyg_lib-0.2.0%2Bpt20cpu-cp310-cp310-linux_x86_64.whl (627 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m627.0/627.0 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_scatter\n","  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_scatter-2.1.1%2Bpt20cpu-cp310-cp310-linux_x86_64.whl (504 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.1/504.1 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_sparse\n","  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_sparse-0.6.17%2Bpt20cpu-cp310-cp310-linux_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_cluster\n","  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_cluster-1.6.1%2Bpt20cpu-cp310-cp310-linux_x86_64.whl (732 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m732.3/732.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_spline_conv\n","  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_spline_conv-1.2.2%2Bpt20cpu-cp310-cp310-linux_x86_64.whl (205 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.7/205.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.10.1)\n","Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch_sparse) (1.22.4)\n","Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n","Successfully installed pyg_lib-0.2.0+pt20cpu torch_cluster-1.6.1+pt20cpu torch_scatter-2.1.1+pt20cpu torch_sparse-0.6.17+pt20cpu torch_spline_conv-1.2.2+pt20cpu\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement warnings (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for warnings\u001b[0m\u001b[31m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting dgl\n","  Downloading dgl-1.1.0-cp310-cp310-manylinux1_x86_64.whl (5.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.22.4)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.10.1)\n","Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.65.0)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4)\n","Installing collected packages: dgl\n","Successfully installed dgl-1.1.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting texttable\n","  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n","Installing collected packages: texttable\n","Successfully installed texttable-1.6.7\n","cpu\n"]}],"source":["import os\n","import torch\n","os.environ['TORCH'] = torch.__version__\n","print(torch.__version__)\n","\n","#!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n","#!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n","!pip install warnings\n","!pip install dgl\n","!pip install texttable\n","\n","if torch.cuda.is_available():\n","  device = torch.device(\"cuda\")\n","else:\n","  device = torch.device(\"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1684411702741,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"},"user_tz":-720},"id":"dq-Zd6aQWRvb","outputId":"266aff13-2aea-46c8-900f-ceb493ba7e59"},"outputs":[{"data":{"text/plain":["device(type='cpu')"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cGqoLa84WV7k"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2598,"status":"ok","timestamp":1687083877802,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"},"user_tz":-720},"id":"LtHqVCYeNmmv"},"outputs":[],"source":["import copy\n","import torch\n","from torch_geometric.data import DataLoader\n","\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.nn import ARMAConv\n","\n","import os\n","import numpy as np\n","import os.path as osp\n","from torch.autograd import grad\n","from datetime import datetime\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.nn import LEConv, BatchNorm, fps\n","\n","from datetime import datetime\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1687083877803,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"},"user_tz":-720},"id":"G0eqa0s54oKO"},"outputs":[],"source":["import numpy as np\n","import os.path as osp\n","import pickle\n","import torch\n","import torch.utils\n","import torch.utils.data\n","import torch.nn.functional as F\n","from scipy.spatial.distance import cdist\n","from torch_geometric.utils import dense_to_sparse\n","from torch_geometric.data import InMemoryDataset,Data\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1687083877803,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"},"user_tz":-720},"id":"Q2RkYPa8k9nE"},"outputs":[],"source":["import torch\n","from torch.nn import Parameter\n","from torch_scatter import scatter_add\n","from torch_geometric.nn.conv import MessagePassing\n","from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n","from torch_geometric.nn.inits import glorot, zeros\n","import pdb\n","\n","class GCNConv(MessagePassing):\n","\n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 improved=False,\n","                 cached=False,\n","                 bias=True,\n","                 edge_norm=True,\n","                 gfn=False):\n","        super(GCNConv, self).__init__('add')\n","\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.improved = improved\n","        self.cached = cached\n","        self.cached_result = None\n","        self.edge_norm = edge_norm\n","        self.gfn = gfn\n","        self.message_mask = None\n","        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n","\n","        if bias:\n","            self.bias = Parameter(torch.Tensor(out_channels))\n","        else:\n","            self.register_parameter('bias', None)\n","\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        glorot(self.weight)\n","        zeros(self.bias)\n","        self.cached_result = None\n","\n","    @staticmethod\n","    def norm(edge_index, num_nodes, edge_weight, improved=False, dtype=None):\n","        if edge_weight is None:\n","            edge_weight = torch.ones((edge_index.size(1), ),\n","                                     dtype=dtype,\n","                                     device=edge_index.device)\n","\n","        edge_weight = edge_weight.view(-1)\n","\n","\n","        assert edge_weight.size(0) == edge_index.size(1)\n","\n","        edge_index, edge_weight = remove_self_loops(edge_index, edge_weight)\n","        edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n","        # Add edge_weight for loop edges.\n","        loop_weight = torch.full((num_nodes, ),\n","                                 1 if not improved else 2,\n","                                 dtype=edge_weight.dtype,\n","                                 device=edge_weight.device)\n","        edge_weight = torch.cat([edge_weight, loop_weight], dim=0)\n","\n","        row, col = edge_index\n","        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n","        deg_inv_sqrt = deg.pow(-0.5)\n","        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n","\n","        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n","\n","    def forward(self, x, edge_index, edge_weight=None):\n","        \"\"\"\"\"\"\n","\n","        x = torch.matmul(x, self.weight)\n","        if self.gfn:\n","            return x\n","\n","        if not self.cached or self.cached_result is None:\n","            if self.edge_norm:\n","                edge_index, norm = GCNConv.norm(\n","                    edge_index,\n","                    x.size(0),\n","                    edge_weight,\n","                    self.improved,\n","                    x.dtype)\n","            else:\n","                norm = None\n","            self.cached_result = edge_index, norm\n","\n","        edge_index, norm = self.cached_result\n","        return self.propagate(edge_index, x=x, norm=norm)\n","\n","    def message(self, x_j, norm):\n","\n","        if self.edge_norm:\n","            return norm.view(-1, 1) * x_j\n","        else:\n","            return x_j\n","\n","    def update(self, aggr_out):\n","        if self.bias is not None:\n","            aggr_out = aggr_out + self.bias\n","        return aggr_out\n","\n","    def __repr__(self):\n","        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n","                                   self.out_channels)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1687083960334,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"},"user_tz":-720},"id":"jCXZmqIT4gvJ"},"outputs":[],"source":["from functools import partial\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import Linear, BatchNorm1d, Sequential, ReLU\n","from torch_geometric.nn import global_mean_pool, global_add_pool, GINConv, GATConv\n","\n","import random\n","import pdb\n","device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n","\n","layers=3\n","with_random=True\n","fc_num=222\n","hidden=32\n","eval_random=False\n","class causalpreCO(torch.nn.Module):\n","  def __init__(self, num_features,num_classes,gfn=False,collapse=False,residual=False,\n","                      res_branch=\"BNConvReLU\",\n","                      global_pool=\"sum\",\n","                      dropout=0,\n","                      edge_norm=True):\n","    super(causalpreCO, self).__init__()\n","    num_conv_layers = layers\n","    self.global_pool = global_add_pool\n","\n","    self.with_random = with_random\n","    GConv = partial(GCNConv, edge_norm=edge_norm, gfn=gfn)\n","    self.bnc = BatchNorm1d(hidden)\n","    self.bno= BatchNorm1d(hidden)\n","    self.context_convs = GConv(hidden, hidden)\n","    self.objects_convs = GConv(hidden, hidden)\n","\n","\n","    hidden_in = num_features\n","    self.num_classes = num_classes\n","    hidden_out = num_classes\n","    self.fc_num = fc_num\n","\n","    self.object_mlp=torch.nn.Sequential(\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden),\n","        ReLU(),\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden_out)\n","    )\n","\n","\n","    # BN initialization.\n","    for m in self.modules():\n","        if isinstance(m, (torch.nn.BatchNorm1d)):\n","            torch.nn.init.constant_(m.weight, 1)\n","            torch.nn.init.constant_(m.bias,0.0001)\n","\n","  def forward(self,causal,edge_index,causal_edge_weight,batch,eval_random=True):\n","\n","\n","    xo = F.relu(self.objects_convs(self.bno(causal), edge_index, causal_edge_weight))\n","\n","    #xc_logis = self.context_readout_layer(xc)\n","    #xo_logis = self.objects_readout_layer(xo)\n","    #xco_logis = self.random_readout_layer(xc, xo, eval_random=eval_random)\n","\n","    #return xc_logis, xo_logis, xco_logis\n","    return self.objects_readout_layer(xo,batch)\n","\n","\n","\n","\n","  def objects_readout_layer(self,x,batch):\n","      xo = self.global_pool(x, batch)\n","      x_logis = F.log_softmax(self.object_mlp(xo), dim=-1)\n","      return x_logis\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":844,"status":"ok","timestamp":1687083963565,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"},"user_tz":-720},"id":"yeGyMs6CXHzH"},"outputs":[],"source":["class causalpreNO(torch.nn.Module):\n","  def __init__(self, num_features,num_classes,gfn=False,collapse=False,residual=False,\n","                      res_branch=\"BNConvReLU\",\n","                      global_pool=\"sum\",\n","                      dropout=0,\n","                      edge_norm=True):\n","    super(causalpreNO, self).__init__()\n","    num_conv_layers = layers\n","\n","    self.global_pool = global_add_pool\n","\n","    self.with_random = with_random\n","\n","    GConv = partial(GCNConv, edge_norm=edge_norm, gfn=gfn)\n","\n","    hidden_in = num_features\n","    self.num_classes = num_classes\n","    hidden_out = num_classes\n","    self.fc_num = fc_num\n","    self.bnc = BatchNorm1d(hidden)\n","    self.bno= BatchNorm1d(hidden)\n","    self.context_convs = GConv(hidden, hidden)\n","    self.objects_convs = GConv(hidden, hidden)\n","\n","    self.context_mlp=torch.nn.Sequential(\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden),\n","        ReLU(),\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden_out)\n","    )\n","\n","    self.random_readout_mlp=torch.nn.Sequential(\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden),\n","        ReLU(),\n","        BatchNorm1d(hidden),\n","        Linear(hidden, hidden_out)\n","    )\n","    #else:\n","      #   assert False\n","\n","    # BN initialization.\n","    for m in self.modules():\n","        if isinstance(m, (torch.nn.BatchNorm1d)):\n","            torch.nn.init.constant_(m.weight, 1)\n","            torch.nn.init.constant_(m.bias, 0.0001)\n","\n","  #def forward(self,causal,noncausal,batch,causal_edge_weight,noncausal_edge_weight,edge_index,eval_random=True):\n","  def forward(self,causal,noncausal,edge_index,causal_edge_weight,noncausal_edge_weight,batch,eval_random=True):\n","\n","    xc = F.relu(self.context_convs(self.bnc(noncausal), edge_index, noncausal_edge_weight))\n","    xo = F.relu(self.objects_convs(self.bno(causal), edge_index, causal_edge_weight))\n","\n","    xc = self.global_pool(xc, batch)\n","    xo = self.global_pool(xo, batch)\n","\n","    xc_logis = self.context_readout_layer(xc)\n","    #xo_logis = self.objects_readout_layer(xo)\n","    xco_logis = self.random_readout_layer(xc, xo, eval_random=eval_random)\n","\n","    return xc_logis, xco_logis\n","\n","\n","  def context_readout_layer(self, x):\n","\n","\n","      x_logis = F.log_softmax(self.context_mlp(x), dim=-1)\n","      return x_logis\n","\n","  def random_readout_layer(self, xc, xo, eval_random):\n","\n","      num = xc.shape[0]\n","      l = [i for i in range(num)]\n","      if self.with_random:\n","          if eval_random:\n","              random.shuffle(l)\n","      random_idx = torch.tensor(l)\n","      #if self.args.cat_or_add == \"cat\":\n","      #    x = torch.cat((xc[random_idx], xo), dim=1)\n","      #else:\n","      x = xc[random_idx] + xo\n","\n","      x_logis = F.log_softmax(self.random_readout_mlp(x), dim=-1)\n","      return x_logis\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1687083970837,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"},"user_tz":-720},"id":"64_EwVWmnugN"},"outputs":[],"source":["\n","class CausalGAN(torch.nn.Module):\n","    \"\"\"GCN with BN and residual connection.\"\"\"\n","    def __init__(self, num_features,\n","                       num_classes,\n","                       gfn=False,\n","                       collapse=False,\n","                       residual=False,\n","                       res_branch=\"BNConvReLU\",\n","                       global_pool=\"sum\",\n","                       dropout=0.2,\n","                       edge_norm=True):\n","        super(CausalGAN, self).__init__()\n","        num_conv_layers = layers\n","        #hidden = args.hidden\n","        #self.args = args\n","        hidden=32\n","        head=4\n","        self.global_pool = global_add_pool\n","        self.dropout = dropout\n","        self.with_random = with_random\n","        self.without_node_attention = False\n","        self.without_edge_attention = False\n","        GConv = partial(GCNConv, edge_norm=edge_norm, gfn=gfn)\n","\n","        hidden_in = num_features\n","        self.num_classes = num_classes\n","        hidden_out = num_classes\n","        self.fc_num = fc_num\n","        self.bn_feat = BatchNorm1d(hidden_in)\n","        self.conv_feat = GCNConv(hidden_in, hidden, gfn=True) # linear transform\n","        self.bns_conv = torch.nn.ModuleList()\n","        self.convs = torch.nn.ModuleList()\n","\n","        for i in range(num_conv_layers):\n","            self.bns_conv.append(BatchNorm1d(hidden))\n","            self.convs.append(GATConv(hidden, int(hidden / head), heads=head, dropout=dropout))\n","\n","        self.edge_att_mlp = nn.Linear(hidden * 2, 2)\n","        self.node_att_mlp = nn.Linear(hidden, 2)\n","        self.bnc = BatchNorm1d(hidden)\n","        self.bno= BatchNorm1d(hidden)\n","        self.context_convs = GConv(hidden, hidden)\n","        self.objects_convs = GConv(hidden, hidden)\n","        self.relu = nn.ReLU(inplace=False)\n","\n","        for m in self.modules():\n","            if isinstance(m, (torch.nn.BatchNorm1d)):\n","                torch.nn.init.constant_(m.weight, 1)\n","                torch.nn.init.constant_(m.bias, 0.0001)\n","\n","    def forward(self, data, eval_random=True):\n","\n","        x = data.x if data.x is not None else data.feat\n","        edge_index,edge_attr,batch = data.edge_index,data.edge_attr,data.batch\n","        row, col = edge_index\n","        x = self.bn_feat(x)\n","        x = self.relu(self.conv_feat(x, edge_index,edge_weight=edge_attr))\n","\n","        for i, conv in enumerate(self.convs):\n","            x = self.bns_conv[i](x)\n","            x = self.relu(conv(x, edge_index))\n","\n","        edge_rep = torch.cat([x[row], x[col]], dim=-1)\n","\n","        if self.without_edge_attention:\n","            edge_att = 0.5 * torch.ones(edge_rep.shape[0], 2).to(device)\n","        else:\n","            edge_att = F.softmax(self.edge_att_mlp(edge_rep), dim=-1)\n","        edge_weight_c = edge_att[:, 0]\n","        edge_weight_o = edge_att[:, 1]\n","\n","        if self.without_node_attention:\n","            node_att = 0.5 * torch.ones(x.shape[0], 2).to(device)\n","        else:\n","            node_att = F.softmax(self.node_att_mlp(x), dim=-1)\n","        xc = node_att[:, 0].view(-1, 1) * x\n","        xo = node_att[:, 1].view(-1, 1) * x\n","\n","        #print(edge_weight_o)\n","        #xc = F.relu(self.context_convs(self.bnc(xc), edge_index, edge_weight_c))\n","        #xo = F.relu(self.objects_convs(self.bno(xo), edge_index, edge_weight_o))\n","        #node_of_graph=xo.shape[0]\n","        #causal_part,causal_node=split_graph(graph_x=xo,node_of_graph=node_of_graph)\n","        #noncausal_part,noncausal_node=split_graph(graph_x=xc,node_of_graph=node_of_graph,type_of_graph=False)\n","\n","        #xc = self.global_pool(xc, batch)\n","        #xo = self.global_pool(xo, batch)\n","\n","        #xc_logis = self.context_readout_layer(xc)\n","        #xo_logis = self.objects_readout_layer(xo)\n","        #xco_logis = self.random_readout_layer(xc, xo, eval_random=eval_random)\n","\n","      # return (xo,edge_index,edge_weight_o),(xc,edge_index,edge_weight_c),batch\n","        return (xo,edge_weight_o),(xc,edge_weight_c),edge_index,batch\n","        #return (causal_attention,causal_part,causal_node),(noncausal_attention,noncausal_part,noncausal_node),batch\n","        #return (xo,causal_part,causal_node),(xc,noncausal_part,noncausal_node),batch\n","\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":1257,"status":"ok","timestamp":1687083975621,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"},"user_tz":-720},"id":"85yBn66PB_QG"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.datasets import Planetoid\n","from torch_geometric.nn import GCNConv\n","from torch_geometric.data import DataLoader\n","\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from drive.MyDrive.gsst2.graphsst2_dataset import get_dataset, get_dataloader\n","import networkx as nx\n","from torch_geometric.utils import to_networkx\n","def generate_node_feature(data):\n","  G = to_networkx(data, to_undirected=True) # Convert to networkx graph\n","\n","  # Calculate eigenvector centrality\n","  centrality = nx.eigenvector_centrality_numpy(G)\n","\n","  # Convert the centrality dictionary to a list maintaining the order of nodes\n","  node_features = [centrality[node] for node in G.nodes]\n","\n","  # Convert to tensor\n","  node_features=torch.tensor(node_features, dtype=torch.float).view(-1, 1)\n","\n","  return node_features"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":117049,"status":"ok","timestamp":1687084094061,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"},"user_tz":-720},"id":"ewtiaFv7H4AC","outputId":"fff383d3-ec0f-4b11-c863-1dd34c49e6a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset: COLLAB(5000):\n","======================\n","Number of graphs: 5000\n","Number of features: 0\n","Number of classes: 3\n","\n","First graph in the dataset:\n","======================\n","Number of nodes: 52\n","Number of edges: 2188\n","Average node degree: 42.08\n","Contains isolated nodes: No\n","Contains self-loops: No\n","Is undirected: Yes\n","training size:3500\n","valating size:750\n","testing size:750\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'contains_isolated_nodes' is deprecated, use 'has_isolated_nodes' instead\n","  warnings.warn(out)\n","/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'contains_self_loops' is deprecated, use 'has_self_loops' instead\n","  warnings.warn(out)\n"]},{"name":"stdout","output_type":"stream","text":["training size:3500\n","valating size:750\n","testing size:750\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n","  warnings.warn(out)\n"]}],"source":["from torch_geometric.datasets import TUDataset\n","import os.path as osp\n","from torch.utils.data import random_split\n","from torch_geometric.datasets import TUDataset\n","\n","path ='/content/drive/MyDrive/'\n","# Load the COLLAB dataset\n","dataset = TUDataset(root='/content/drive/MyDrive/', name='COLLAB')\n","\n","# Print some information about the dataset\n","print(f'Dataset: {dataset}:')\n","print('======================')\n","print(f'Number of graphs: {len(dataset)}')\n","print(f'Number of features: {dataset.num_features}')\n","print(f'Number of classes: {dataset.num_classes}')\n","\n","# Print information about the first graph in the dataset\n","data = dataset[1]\n","print('\\nFirst graph in the dataset:')\n","print('======================')\n","print(f'Number of nodes: {data.num_nodes}')\n","print(f'Number of edges: {data.num_edges}')\n","print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n","print(f'Contains isolated nodes: {\"Yes\" if data.contains_isolated_nodes() else \"No\"}')\n","print(f'Contains self-loops: {\"Yes\" if data.contains_self_loops() else \"No\"}')\n","print(f'Is undirected: {\"Yes\" if data.is_undirected() else \"No\"}')\n","\n","train_size = int(len(dataset) * 0.7)\n","val_size = int(len(dataset) * 0.15)\n","test_size = len(dataset) - train_size - val_size\n","print(f'training size:{train_size}')\n","print(f'valating size:{val_size}')\n","print(f'testing size:{test_size}')\n","\n","# Generate the splits\n","train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n","training=[]\n","valating=[]\n","testing=[]\n","for data in train_dataset:\n","  data.x=generate_node_feature(data)\n","  training.append(data)\n","for data in val_dataset:\n","  data.x=generate_node_feature(data)\n","  valating.append(data)\n","for data in test_dataset:\n","  data.x=generate_node_feature(data)\n","  testing.append(data)\n","print(f'training size:{len(training)}')\n","print(f'valating size:{len(valating)}')\n","print(f'testing size:{len(testing)}')\n","train_loader = DataLoader(training, batch_size=32, shuffle=True)\n","val_loader = DataLoader(valating, batch_size=32, shuffle=False)\n","test_loader = DataLoader(testing, batch_size=1, shuffle=False)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1687084094062,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"},"user_tz":-720},"id":"oqj2r-DqOxaT"},"outputs":[],"source":["feat_dim=1\n","num_classes = 3\n","causal_model2 = CausalGAN(feat_dim,num_classes).to(device)\n","predictno_model2=causalpreNO(feat_dim,num_classes).to(device)\n","predictco_model2=causalpreCO(feat_dim,num_classes).to(device)\n","model_optimizer2 = torch.optim.Adam(\n","            list(causal_model2.parameters()) +\n","            list(predictno_model2.parameters())+list(predictco_model2.parameters()),\n","            lr=0.001)\n","Epo=500\n","lr_scheduler = CosineAnnealingLR(model_optimizer2, T_max=Epo, eta_min=1e-6, last_epoch=-1, verbose=False)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2637167,"status":"ok","timestamp":1687086740788,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"},"user_tz":-720},"id":"NvidGrhFBdK4","outputId":"36c83d54-670e-43a7-9be8-bf8ae20869ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["-----training-------0\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [0/500]: 100%|██████████| 110/110 [01:44<00:00,  1.05it/s, loss=1.21]\n"]},{"name":"stdout","output_type":"stream","text":["number of 0 with total loss:1.4154795375737277\n","------valation---------0\n","causal val accuracy:0.6533333333333333\n","-----training-------1\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [1/500]: 100%|██████████| 110/110 [01:44<00:00,  1.05it/s, loss=1.24]\n"]},{"name":"stdout","output_type":"stream","text":["number of 1 with total loss:1.266071183573116\n","------valation---------1\n","causal val accuracy:0.6466666666666666\n","-----training-------2\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [2/500]: 100%|██████████| 110/110 [01:43<00:00,  1.06it/s, loss=1.31]\n"]},{"name":"stdout","output_type":"stream","text":["number of 2 with total loss:1.2139023813334378\n","------valation---------2\n","causal val accuracy:0.6613333333333333\n","-----training-------3\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [3/500]: 100%|██████████| 110/110 [01:41<00:00,  1.08it/s, loss=1.14]\n"]},{"name":"stdout","output_type":"stream","text":["number of 3 with total loss:1.188320971618999\n","------valation---------3\n","causal val accuracy:0.6666666666666666\n","-----training-------4\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [4/500]: 100%|██████████| 110/110 [01:44<00:00,  1.06it/s, loss=0.948]\n"]},{"name":"stdout","output_type":"stream","text":["number of 4 with total loss:1.1363106765530326\n","------valation---------4\n","causal val accuracy:0.6626666666666666\n","-----training-------5\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [5/500]: 100%|██████████| 110/110 [01:46<00:00,  1.03it/s, loss=1.55]\n"]},{"name":"stdout","output_type":"stream","text":["number of 5 with total loss:1.1306412929838354\n","------valation---------5\n","causal val accuracy:0.6573333333333333\n","-----training-------6\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [6/500]: 100%|██████████| 110/110 [01:57<00:00,  1.07s/it, loss=1.07]\n"]},{"name":"stdout","output_type":"stream","text":["number of 6 with total loss:1.1086786183443937\n","------valation---------6\n","causal val accuracy:0.6746666666666666\n","-----training-------7\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [7/500]: 100%|██████████| 110/110 [01:51<00:00,  1.01s/it, loss=0.999]\n"]},{"name":"stdout","output_type":"stream","text":["number of 7 with total loss:1.0881438255310059\n","------valation---------7\n","causal val accuracy:0.6613333333333333\n","-----training-------8\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [8/500]: 100%|██████████| 110/110 [01:48<00:00,  1.01it/s, loss=1.09]\n"]},{"name":"stdout","output_type":"stream","text":["number of 8 with total loss:1.0709687780250203\n","------valation---------8\n","causal val accuracy:0.6733333333333333\n","-----training-------9\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [9/500]: 100%|██████████| 110/110 [01:47<00:00,  1.02it/s, loss=1.2]\n"]},{"name":"stdout","output_type":"stream","text":["number of 9 with total loss:1.046335063197396\n","------valation---------9\n","causal val accuracy:0.6626666666666666\n","-----training-------10\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [10/500]: 100%|██████████| 110/110 [01:48<00:00,  1.01it/s, loss=1.31]\n"]},{"name":"stdout","output_type":"stream","text":["number of 10 with total loss:1.0569578512148423\n","------valation---------10\n","causal val accuracy:0.6906666666666667\n","-----training-------11\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [11/500]: 100%|██████████| 110/110 [01:53<00:00,  1.03s/it, loss=1.13]\n"]},{"name":"stdout","output_type":"stream","text":["number of 11 with total loss:1.043600411306728\n","------valation---------11\n","causal val accuracy:0.684\n","-----training-------12\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [12/500]: 100%|██████████| 110/110 [01:44<00:00,  1.05it/s, loss=1.07]\n"]},{"name":"stdout","output_type":"stream","text":["number of 12 with total loss:1.0206576759164983\n","------valation---------12\n","causal val accuracy:0.692\n","-----training-------13\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [13/500]: 100%|██████████| 110/110 [01:42<00:00,  1.07it/s, loss=1.18]\n"]},{"name":"stdout","output_type":"stream","text":["number of 13 with total loss:1.003166060556065\n","------valation---------13\n","causal val accuracy:0.7053333333333334\n","-----training-------14\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [14/500]: 100%|██████████| 110/110 [01:45<00:00,  1.04it/s, loss=1.07]\n"]},{"name":"stdout","output_type":"stream","text":["number of 14 with total loss:1.0027103462002493\n","------valation---------14\n","causal val accuracy:0.6866666666666666\n","-----training-------15\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [15/500]: 100%|██████████| 110/110 [01:50<00:00,  1.00s/it, loss=1.14]\n"]},{"name":"stdout","output_type":"stream","text":["number of 15 with total loss:0.9950575216249986\n","------valation---------15\n","causal val accuracy:0.6786666666666666\n","-----training-------16\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [16/500]: 100%|██████████| 110/110 [01:50<00:00,  1.01s/it, loss=0.895]\n"]},{"name":"stdout","output_type":"stream","text":["number of 16 with total loss:0.9807486425746571\n","------valation---------16\n","causal val accuracy:0.7106666666666667\n","-----training-------17\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [17/500]: 100%|██████████| 110/110 [01:52<00:00,  1.02s/it, loss=1.46]\n"]},{"name":"stdout","output_type":"stream","text":["number of 17 with total loss:0.9865031312812459\n","------valation---------17\n","causal val accuracy:0.6746666666666666\n","-----training-------18\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [18/500]: 100%|██████████| 110/110 [01:48<00:00,  1.01it/s, loss=1.33]\n"]},{"name":"stdout","output_type":"stream","text":["number of 18 with total loss:0.966531777381897\n","------valation---------18\n","causal val accuracy:0.716\n","-----training-------19\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [19/500]: 100%|██████████| 110/110 [01:47<00:00,  1.03it/s, loss=1.27]\n"]},{"name":"stdout","output_type":"stream","text":["number of 19 with total loss:0.9542289186607708\n","------valation---------19\n","causal val accuracy:0.716\n","-----training-------20\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [20/500]: 100%|██████████| 110/110 [01:49<00:00,  1.00it/s, loss=1.58]\n"]},{"name":"stdout","output_type":"stream","text":["number of 20 with total loss:0.9575496359304948\n","------valation---------20\n","causal val accuracy:0.716\n","-----training-------21\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [21/500]: 100%|██████████| 110/110 [01:49<00:00,  1.00it/s, loss=0.862]\n"]},{"name":"stdout","output_type":"stream","text":["number of 21 with total loss:0.9545520359819586\n","------valation---------21\n","causal val accuracy:0.7013333333333334\n"]}],"source":["import time\n","import json\n","\n","loss_value=[]\n","loss_value_valation=[]\n","c=0.5\n","o=1\n","co=0.5\n","def num_graphs(data):\n","  if data.batch is not None:\n","      return data.num_graphs\n","  else:\n","      return data.x.size(0)\n","from tqdm import tqdm\n","Epo=500\n","for epoch in range(Epo):\n","  #model.train()\n","  causal_model2.train()\n","  predictno_model2.train()\n","  predictco_model2.train()\n","  total_loss = 0\n","  total_loss_c = 0\n","  total_loss_o = 0\n","  total_loss_co = 0\n","  correct_o = 0\n","  nb=0\n","  print(f\"-----training-------{epoch}\")\n","  loop = tqdm(enumerate(train_loader),total=len(train_loader))\n","  for it, data in loop:\n","#  for it, data in enumerate(train_loader):\n","      nb+=1\n","      model_optimizer2.zero_grad()\n","      data = data.to(device)\n","\n","      one_hot_target = data.y.view(-1)\n","      #causal,noncausal,batch=causal_model(data)\n","      (xo,edge_weight_o),(xc,edge_weight_c),edge_index,batch=causal_model2(data)\n","      #c_logs, o_logs, co_logs = predict_model(causal=causal,noncausal=noncausal,batch=batch,causal_edge_weight=edge_weight_causal,noncausal_edge_weight=edge_weight_noncausal,edge_index=edge_index)\n","      c_logs,co_logs = predictno_model2(causal=xo,noncausal=xc,edge_index=edge_index,causal_edge_weight=edge_weight_o,noncausal_edge_weight=edge_weight_c,batch=batch)\n","      o_logs=predictco_model2(causal=xo,edge_index=edge_index,causal_edge_weight=edge_weight_o,batch=batch)\n","      uniform_target = torch.ones_like(c_logs, dtype=torch.float).to(device)/num_classes\n","     # sim=F.cosine_similarity(causal_attention,noncausal_attention).mean()\n","      #print(o_logs)\n","      #print(type(o_logs))\n","      c_loss = F.kl_div(c_logs, uniform_target,reduction='batchmean')\n","      #print(f'causal loss{c_loss}')\n","      o_loss = F.nll_loss(o_logs,one_hot_target)\n","      #print(f'noncausal loss{o_loss}')\n","      co_loss = F.nll_loss(co_logs,one_hot_target)\n","      #print(f'com causal loss{co_loss}')\n","      loss = c * c_loss + o * o_loss + co * co_loss\n","\n","      pred_o = o_logs.max(1)[1]\n","      correct_o += pred_o.eq(data.y.view(-1)).sum().item()\n","      loss.backward()\n","      total_loss += loss.item() #* num_graphs(data)\n","      total_loss_c += c_loss.item() #* num_graphs(data)\n","      total_loss_o += o_loss.item() #* num_graphs(data)\n","      total_loss_co += co_loss.item()# * num_graphs(data)\n","      model_optimizer2.step()\n","      loop.set_description(f\"Epoch [{epoch}/{Epo}]\")\n","      loop.set_postfix(loss = loss.item())\n","\n","  #num = len(train_loader.dataset)\n","  total_loss = total_loss / nb\n","  total_loss_c = total_loss_c / nb\n","  print(f'number of {epoch} with total loss:{total_loss}')\n","  loss_value.append(total_loss)\n","  total_loss_o = total_loss_o / nb\n","  total_loss_co = total_loss_co / nb\n","  correct_o = correct_o / nb\n","  lr_scheduler.step()\n","  with torch.no_grad():\n","    correct=0\n","    correct_c=0\n","    correct_o=0\n","    print(f\"------valation---------{epoch}\")\n","    causal_model2.eval()\n","    predictno_model2.eval()\n","    predictco_model2.eval()\n","    for data in val_loader:\n","      (xo,edge_weight_o),(xc,edge_weight_c),edge_index,batch=causal_model2(data)\n","      #c_logs, o_logs, co_logs = predict_model(causal=causal,noncausal=noncausal,batch=batch,causal_edge_weight=edge_weight_causal,noncausal_edge_weight=edge_weight_noncausal,edge_index=edge_index)\n","      c_logs,co_logs = predictno_model2(causal=xo,noncausal=xc,edge_index=edge_index,causal_edge_weight=edge_weight_o,noncausal_edge_weight=edge_weight_c,batch=batch)\n","      o_logs=predictco_model2(causal=xo,edge_index=edge_index,causal_edge_weight=edge_weight_o,batch=batch)\n","      pred = co_logs.max(1)[1]\n","      pred_c = c_logs.max(1)[1]\n","      pred_o = o_logs.max(1)[1]\n","      correct += pred.eq(data.y.view(-1)).sum().item()\n","      correct_c += pred_c.eq(data.y.view(-1)).sum().item()\n","      correct_o += pred_o.eq(data.y.view(-1)).sum().item()\n","    acc_co = correct / len(val_loader.dataset)\n","    acc_c = correct_c / len(val_loader.dataset)\n","    acc_o = correct_o / len(val_loader.dataset)\n","    print(f\"causal val accuracy:{acc_co}\")\n","    loss_value_valation.append(acc_co)\n","    dictionary={\"number of epoch\":epoch,\n","                \"training loss list\":loss_value,\n","                \"valation accuracy list\":loss_value_valation}\n","\n","    # Serializing json\n","    json_object = json.dumps(dictionary,indent=3)\n","\n","    # Writing to sample.json\n","   # with open(\"/content/drive/MyDrive/running_RQ2/COLLAB/CAL/CAL_tl_va_e.json\", \"w\") as outfile:\n","   #     outfile.write(json_object)\n","\n","    #torch.save(causal_model.state_dict(), '/content/drive/MyDrive/Colab_Notebooks/430cau_my6000.pt')\n","    #torch.save(predictco_model.state_dict(), '/content/drive/MyDrive/Colab_Notebooks/430caupred_my6000.pt')\n","    #torch.save(predictno_model.state_dict(), '/content/drive/MyDrive/Colab_Notebooks/430noncaupred_my6000.pt')\n","    #torch.save(model_heter.state_dict(), '/content/drive/MyDrive/Colab_Notebooks/430heter6000.pt')\n","   # torch.save({\n","   #         'causal_model.state_dic': causal_model2.state_dict(),\n","   #         'predictco_model_state_dict()': predictco_model2.state_dict(),\n","   #         'predictno_model_state_dict()': predictno_model2.state_dict(),\n","   #         'opt':model_optimizer2.state_dict()\n","   #         }, '/content/drive/MyDrive/running_RQ2/COLLAB/CAL/NLP_allmodel_cal.pt')\n","    if(epoch>20):\n","      check=abs(acc_o-loss_value_valation[len(loss_value_valation)-20])/20\n","      if(check<=0.01):\n","        break\n","\n","\n","  #model_optimizer.zero_grad()\n","  #total_loss.backward()\n","  #model_optimizer.step()\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-qyTG76lB74M"},"outputs":[],"source":["correct=0\n","correct_c=0\n","correct_o=0\n","print(f\"------test---------{0000000}\")\n","causal_model2.eval()\n","predictno_model2.eval()\n","predictco_model2.eval()\n","for data in test_loader:\n","  (xo,edge_weight_o),(xc,edge_weight_c),edge_index,batch=causal_model2(data)\n","  #c_logs, o_logs, co_logs = predict_model(causal=causal,noncausal=noncausal,batch=batch,causal_edge_weight=edge_weight_causal,noncausal_edge_weight=edge_weight_noncausal,edge_index=edge_index)\n","  c_logs,co_logs = predictno_model2(causal=xo,noncausal=xc,edge_index=edge_index,causal_edge_weight=edge_weight_o,noncausal_edge_weight=edge_weight_c,batch=batch)\n","  o_logs=predictco_model2(causal=xo,edge_index=edge_index,causal_edge_weight=edge_weight_o,batch=batch)\n","  pred = co_logs.max(1)[1]\n","  pred_c = c_logs.max(1)[1]\n","  pred_o = o_logs.max(1)[1]\n","  correct += pred.eq(data.y.view(-1)).sum().item()\n","  correct_c += pred_c.eq(data.y.view(-1)).sum().item()\n","  correct_o += pred_o.eq(data.y.view(-1)).sum().item()\n","acc_co = correct / len(test_loader.dataset)\n","acc_c = correct_c / len(test_loader.dataset)\n","acc_o = correct_o / len(test_loader.dataset)\n","print(f\"com causal test accuracy:{acc_co}\")\n","print(f\"causal test accuracy:{acc_o}\")\n","dictionary={\"number of epoch\":epoch,\n","            \"training loss list\":loss_value,\n","            \"valation accuracy list\":loss_value_valation,\n","            \"test accuracy value\":acc_co}\n","\n","# Serializing json\n","json_object = json.dumps(dictionary,indent=4)\n","\n","# Writing to sample.json\n","with open(\"/content/drive/MyDrive/running_RQ2/COLLAB/CAL/CAL_tl_va_e.json\", \"w\") as outfile:\n","    outfile.write(json_object)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1684414571035,"user":{"displayName":"LI-CHENG YEH","userId":"17628358221149984922"},"user_tz":-720},"id":"tSV_5LT74ACj","outputId":"93b8d4db-b9f6-43cd-d703-3a0b3fc79e55"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'test_loader = DataLoader(testing_final, batch_size=1, shuffle=False)\\n\\ncorrect=0\\ncorrect_c=0\\ncorrect_o=0\\nprint(f\"------test---------{0000000}\")\\ncausal_model2.eval()\\npredictno_model2.eval()\\npredictco_model2.eval()\\nfor data in test_loader:\\n  (xo,edge_weight_o),(xc,edge_weight_c),edge_index,batch=causal_model2(data)\\n  #c_logs, o_logs, co_logs = predict_model(causal=causal,noncausal=noncausal,batch=batch,causal_edge_weight=edge_weight_causal,noncausal_edge_weight=edge_weight_noncausal,edge_index=edge_index)\\n  c_logs,co_logs = predictno_model2(causal=xo,noncausal=xc,edge_index=edge_index,causal_edge_weight=edge_weight_o,noncausal_edge_weight=edge_weight_c,batch=batch)\\n  o_logs=predictco_model2(causal=xo,edge_index=edge_index,causal_edge_weight=edge_weight_o,batch=batch)\\n  pred = co_logs.max(1)[1]\\n  pred_c = c_logs.max(1)[1] \\n  pred_o = o_logs.max(1)[1]\\n  correct += pred.eq(data.y.view(-1)).sum().item()\\n  correct_c += pred_c.eq(data.y.view(-1)).sum().item()\\n  correct_o += pred_o.eq(data.y.view(-1)).sum().item()\\nacc_co = correct / len(test_loader.dataset)\\nacc_c = correct_c / len(test_loader.dataset)\\nacc_o = correct_o / len(test_loader.dataset)\\nprint(f\"com causal test accuracy:{acc_co}\")\\nprint(f\"causal test accuracy:{acc_o}\")\\n#dictionary={\"number of epoch\":epoch,\\n#            \"training loss list\":loss_value,\\n#            \"valation accuracy list\":loss_value_valation,\\n#            \"test accuracy value\":acc_co}'"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["'''test_loader = DataLoader(testing_final, batch_size=1, shuffle=False)\n","\n","correct=0\n","correct_c=0\n","correct_o=0\n","print(f\"------test---------{0000000}\")\n","causal_model2.eval()\n","predictno_model2.eval()\n","predictco_model2.eval()\n","for data in test_loader:\n","  (xo,edge_weight_o),(xc,edge_weight_c),edge_index,batch=causal_model2(data)\n","  #c_logs, o_logs, co_logs = predict_model(causal=causal,noncausal=noncausal,batch=batch,causal_edge_weight=edge_weight_causal,noncausal_edge_weight=edge_weight_noncausal,edge_index=edge_index)\n","  c_logs,co_logs = predictno_model2(causal=xo,noncausal=xc,edge_index=edge_index,causal_edge_weight=edge_weight_o,noncausal_edge_weight=edge_weight_c,batch=batch)\n","  o_logs=predictco_model2(causal=xo,edge_index=edge_index,causal_edge_weight=edge_weight_o,batch=batch)\n","  pred = co_logs.max(1)[1]\n","  pred_c = c_logs.max(1)[1]\n","  pred_o = o_logs.max(1)[1]\n","  correct += pred.eq(data.y.view(-1)).sum().item()\n","  correct_c += pred_c.eq(data.y.view(-1)).sum().item()\n","  correct_o += pred_o.eq(data.y.view(-1)).sum().item()\n","acc_co = correct / len(test_loader.dataset)\n","acc_c = correct_c / len(test_loader.dataset)\n","acc_o = correct_o / len(test_loader.dataset)\n","print(f\"com causal test accuracy:{acc_co}\")\n","print(f\"causal test accuracy:{acc_o}\")\n","#dictionary={\"number of epoch\":epoch,\n","#            \"training loss list\":loss_value,\n","#            \"valation accuracy list\":loss_value_valation,\n","#            \"test accuracy value\":acc_co}'''\n","\n","# Serializing json\n","#json_object = json.dumps(dictionary,indent=4)\n","\n","# Writing to sample.json\n","#with open(\"/content/drive/MyDrive/running_cal_mnist/number_tl_va_e095.json\", \"w\") as outfile:\n","#    outfile.write(json_object)'''\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyP7yT86nFf4AYm8MMFeVKB5","mount_file_id":"1rPRdVFxHfxZYHHcoEla2T3JRInv2cx-L","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
